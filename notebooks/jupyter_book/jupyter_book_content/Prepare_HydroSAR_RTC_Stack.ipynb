{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare a SAR RTC Data Stack for HydroSAR\n",
    "\n",
    "**Joseph H Kennedy and Alex Lewandowski; Alaska Satellite Facility**\n",
    "\n",
    "This notebook downloads an ASF-HyP3 RTC project or OPERA-S1 RTCs and prepares a SAR data stack for use with HydroSAR.\n",
    "\n",
    "The stack may be comprised of a single RTC or be a deep, multi-temporal time series.\n",
    "\n",
    "The notebook will:\n",
    "- project all data in the stack to the predominant EPSG\n",
    "- Merge scenes acquired on the same day to create large spatial mosaics\n",
    "\n",
    "If using HyP3 data, this notebook assumes that you have already ordered an RTC stack over your area of interest using the [Alaska Satellite Facility's](https://www.asf.alaska.edu/) value-added product system HyP3, available via [ASF Data Search (Vertex)](https://search.asf.alaska.edu/#/), the [HyP3 API](https://hyp3-api.asf.alaska.edu/ui/), or the [hyp3_sdk](https://github.com/ASFHyP3/hyp3-sdk).\n",
    "\n",
    "---\n",
    "\n",
    "## 0. Select or create a working directory for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import opensarlab_lib as osl\n",
    "\n",
    "age = osl.select_parameter(\n",
    "    [\n",
    "        \"Create a new RTC stack\",\n",
    "        \"Add to existing RTC stack\"\n",
    "    ]\n",
    ")\n",
    "display(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "new = 'new' in age.value\n",
    "\n",
    "if new:\n",
    "    print(f'Current working directory: {Path.cwd()}')\n",
    "    print('Create a new directory to hold your data:')\n",
    "    data_path = input(f'Enter an unused path for a new data directory:  {Path.home()}/')\n",
    "    try:\n",
    "        data_path = Path.home() / data_path.strip()\n",
    "        data_path.mkdir()\n",
    "    except:\n",
    "        raise\n",
    "else:\n",
    "    path = Path.home()\n",
    "    fc = FileChooser(path)\n",
    "    print('Select an existing data directory')\n",
    "    display(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not new:\n",
    "    data_path = Path(fc.selected_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Migrate an RTC Stack from ASF\n",
    "\n",
    "**Select a data source and access option**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_access = osl.select_parameter(\n",
    "    [\n",
    "        'HyP3: Access RTC data with any valid HyP3 username and HyP3 RTC Project Name',\n",
    "        'HyP3: Search your Projects for available RTC data',\n",
    "        'OPERA: Search for OPERA S1 L2-RTC data'\n",
    "    ]\n",
    ")\n",
    "display(data_access)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authenticate with HyP3 or gather credentials required to download OPERA data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "\n",
    "from hyp3_sdk import Batch, HyP3\n",
    "\n",
    "hyp3 = 'HyP3' in data_access.value\n",
    "rtc_search = 'available RTC data' in data_access.value\n",
    "opera = 'OPERA' in data_access.value\n",
    "\n",
    "if hyp3:\n",
    "    hyp3_session = HyP3(prompt=True)\n",
    "else:\n",
    "    username = input(\"Enter your EDL username\")\n",
    "    password = getpass(\"Enter your EDL password\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may search for `RTC_GAMMA` projects in your own account or migrate data from any user's account**\n",
    "\n",
    "- Retrieving data from another user's account only requires their username and the project name.\n",
    "- It does **not** require the other user's password. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "from IPython.display import Markdown\n",
    "\n",
    "if hyp3:\n",
    "    product_type = 'RTC_GAMMA'\n",
    "    if rtc_search:\n",
    "        my_hyp3_info = hyp3_session.my_info()\n",
    "        active_projects = dict()\n",
    "        \n",
    "        print(f\"Checking all HyP3 projects for current {product_type} jobs\")\n",
    "        for project in tqdm(my_hyp3_info['job_names']):\n",
    "                batch = Batch()\n",
    "                batch = hyp3_session.find_jobs(\n",
    "                    name=project, \n",
    "                    job_type=product_type\n",
    "                ).filter_jobs(running=False, include_expired=False)\n",
    "                if len(batch) > 0:\n",
    "                    active_projects.update({batch.jobs[0].name: batch})\n",
    "        \n",
    "        if len(active_projects) > 0:\n",
    "            display(Markdown(\"<text style='color:darkred;'>Note: After selecting a project, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "            display(Markdown(\"<text style='color:darkred;'>Otherwise, you will rerun this code cell.</text>\"))\n",
    "            print('\\nSelect a Project:')\n",
    "            project_select = osl.select_parameter(active_projects.keys())\n",
    "            display(project_select)\n",
    "        else:\n",
    "            raise Exception(\"Found no active projects containing RTC products\")\n",
    "    else:\n",
    "        username = input(\"enter the HyP3 username on the account containing an SBAS stack to migrate\")\n",
    "        project_name = input(\"Enter the HyP3 project name\")\n",
    "        batch = Batch()\n",
    "        batch = hyp3_session.find_jobs(\n",
    "            name=project_name, \n",
    "            job_type=product_type, \n",
    "            user_id=username\n",
    "        ).filter_jobs(running=False, include_expired=False)\n",
    "elif opera:\n",
    "    try:\n",
    "        user_pass_session = disco.ASFSession().auth_with_creds(username, password)\n",
    "    except disco.ASFAuthenticationError as e:\n",
    "        print(f'Auth failed: {e}')\n",
    "    else:\n",
    "        print('Successfully started ASF_Search session.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If accessing OPERA RTCs, view the `search` function documentation to see the available parameters of your search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opera:\n",
    "    disco.search?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**If accessing OPERA RTCs, update the search parameters to suit your needs, and search for data**\n",
    "\n",
    "The search options below are serve as an example. \n",
    "\n",
    "Do not change:\n",
    "- `'dataset': 'OPERA-S1'`\n",
    "- `'processingLevel': ['RTC']`\n",
    "\n",
    "Update or add any other parameters from the documentation above to fit your search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opera:\n",
    "    options = {\n",
    "        'intersectsWith': 'POINT(90.4976 24.1595)',\n",
    "        'dataset': 'OPERA-S1',\n",
    "        'start': '2016-07-03T00:00:00Z',\n",
    "        'end': '2024-01-31T00:00:00Z',\n",
    "        'flightDirection': 'ASCENDING',\n",
    "        'processingLevel': ['RTC'],\n",
    "        'maxResults': '1000'\n",
    "    }\n",
    "    results = disco.search(**options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opera:\n",
    "    options = {\n",
    "        'intersectsWith': 'POLYGON((90.253 23.9691,90.4222 23.9691,90.4222 24.1679,90.253 24.1679,90.253 23.9691))',\n",
    "        'dataset': 'OPERA-S1',\n",
    "        'start': '2016-07-03T00:00:00Z',\n",
    "        'end': '2024-01-31T00:00:00Z',\n",
    "        'flightDirection': 'ASCENDING',\n",
    "        'processingLevel': ['RTC'],\n",
    "        'maxResults': '1000'\n",
    "    }\n",
    "    results = disco.search(**options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You can also search for OPERA RTCs with a list of product IDs**\n",
    "\n",
    "Uncomment the code cell below to search using OPERA RTC IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment to search by OPERA RTC ID\n",
    "# if opera:\n",
    "#     product_list = [\n",
    "#         \"OPERA_L2_RTC-S1_T173-370304-IW1_20231006T134412Z_20231007T132700Z_S1A_30_v1.0\",\n",
    "#         \"OPERA_L2_RTC-S1_T173-370304-IW1_20231018T134412Z_20231019T044908Z_S1A_30_v1.0\"\n",
    "#         ]\n",
    "#     results = disco.granule_search(product_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**If accessing HyP3 data, select a date range of products to migrate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3 and rtc_search:\n",
    "    jobs = active_projects[project_select.value]\n",
    "elif hyp3:\n",
    "    jobs = batch\n",
    "\n",
    "if hyp3:\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a date range, you should select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you may simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Date Range:')\n",
    "    dates = osl.get_job_dates(jobs)\n",
    "    date_picker = osl.gui_date_picker(dates)\n",
    "    display(date_picker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If accessing HyP3 data, save the selected date range and remove products falling outside of it:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if hyp3:\n",
    "    date_range = osl.get_slider_vals(date_picker)\n",
    "    date_range[0] = date_range[0].date()\n",
    "    date_range[1] = date_range[1].date()\n",
    "    print(f\"Date Range: {str(date_range[0])} to {str(date_range[1])}\")\n",
    "    batch = osl.filter_jobs_by_date(batch, date_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If accessing HyP3 data, gather the available paths and orbit directions for the remaining products:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3:\n",
    "    display(Markdown(\"<text style='color:darkred;'><text style='font-size:150%;'>This may take some time for projects containing many jobs...</text></text>\"))\n",
    "    osl.set_paths_orbits(jobs)\n",
    "    paths = set()\n",
    "    orbit_directions = set()\n",
    "    for rtc in jobs:\n",
    "        paths.add(rtc.path)\n",
    "        orbit_directions.add(rtc.orbit_direction)\n",
    "    paths.add('All Paths')\n",
    "    display(Markdown(f\"<text style=color:blue><text style='font-size:175%;'>Done.</text></text>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**If accessing HyP3 data, select a path or paths (use shift or ctrl to select multiple paths):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3:\n",
    "    display(Markdown(\"<text style='color:darkred;'>Note: After selecting a path, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "    display(Markdown(\"<text style='color:darkred;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Path:')\n",
    "    path_choice = osl.select_mult_parameters(paths)\n",
    "    display(path_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If accessing HyP3 data, save the selected flight path/s:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3:\n",
    "    flight_path = path_choice.value\n",
    "    if flight_path:\n",
    "        if flight_path:\n",
    "            print(f\"Flight Path: {flight_path}\")\n",
    "        else:\n",
    "            print('Flight Path: All Paths')\n",
    "    else:\n",
    "        print(\"WARNING: You must select a flight path in the previous cell, then rerun this cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If accessing HyP3 data, select an orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3:\n",
    "    if len(orbit_directions) > 1:\n",
    "        display(Markdown(\"<text style='color:red;'>Note: After selecting a flight direction, you must select the next cell before hitting the 'Run' button or typing Shift/Enter.</text>\"))\n",
    "        display(Markdown(\"<text style='color:red;'>Otherwise, you will simply rerun this code cell.</text>\"))\n",
    "    print('\\nSelect a Flight Direction:')\n",
    "    direction_choice = osl.select_parameter(orbit_directions, 'Direction:')\n",
    "    display(direction_choice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If accessing HyP3 data, save the selected orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3:\n",
    "    direction = direction_choice.value\n",
    "    print(f\"Orbit Direction: {direction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If accessing HyP3 data, filter jobs by path and orbit direction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def filter_old_bursts(results):\n",
    "    filtered_bursts = dict()\n",
    "    acquisition_date_regex = r\"(?<=OPERA_L2_RTC-S1_)T\\d{3}-\\d{6}-IW\\d_\\d{8}T\\d{6}Z(?=_\\d{8}T\\d{6}Z)\"\n",
    "    process_date_regex = r\"(?<=OPERA_L2_RTC-S1_T\\d{3}-\\d{6}-IW\\d_\\d{8}T\\d{6}Z_)\\d{8}T\\d{6}Z\"\n",
    "    \n",
    "    for b in results:\n",
    "        rtc_id = b.properties['fileID']\n",
    "        try:\n",
    "            id_date = re.search(acquisition_date_regex, rtc_id).group(0)\n",
    "            try:\n",
    "                # for bursts that only differ by processing date, we can use a simple relational comparison\n",
    "                if filtered_bursts[id_date].properties['fileID'] < rtc_id:\n",
    "                    filtered_bursts[id_date] = b\n",
    "            except KeyError:\n",
    "                filtered_bursts[id_date] = b\n",
    "        except AttributeError:\n",
    "            raise Exception(f\"Acquisition not found in RTC ID: {str(b)}\")\n",
    "            \n",
    "    return list(filtered_bursts.values())   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if hyp3:\n",
    "    jobs = osl.filter_jobs_by_path(jobs, flight_path)\n",
    "    jobs = osl.filter_jobs_by_orbit(jobs, direction)\n",
    "    print(f\"There are {len(jobs)} products to download.\")\n",
    "else:\n",
    "    results = [r for r in results if 'operaBurstID' in r.properties.keys()]\n",
    "    results = filter_old_bursts(results)\n",
    "    print(f\"There are {len(results)} OPERA RTCs to download\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Download the products, unzip them into a directory named after the product type, and delete the zip files:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from asf_search.download.file_download_type import FileDownloadType\n",
    "\n",
    "if hyp3:\n",
    "    print(f\"\\nProject: {jobs.jobs[0].name}\")\n",
    "    project_zips = jobs.download_files(data_path)\n",
    "    for z in project_zips:\n",
    "        osl.asf_unzip(str(data_path), str(z))\n",
    "        z.unlink()\n",
    "else:\n",
    "    for p in results:\n",
    "        p.download(data_path, session=user_pass_session, fileType=FileDownloadType.ALL_FILES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Move the VV GeoTiffs and DEMs to their own directories, and delete unneeded data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "if hyp3:\n",
    "    rtc_dirs = list(data_path.glob('*RTC*'))\n",
    "    vh_paths = list(data_path.glob('*RTC*/*VH*.tif'))\n",
    "    vv_paths = list(data_path.glob('*RTC*/*VV*.tif'))\n",
    "else:\n",
    "    vh_paths = list(data_path.glob('*VH*.tif'))\n",
    "    vv_paths = list(data_path.glob('*VV*.tif'))\n",
    "    \n",
    "vh_dir = data_path / 'VH'\n",
    "vh_dir.mkdir(exist_ok=True)\n",
    "\n",
    "vv_dir = data_path / 'VV'\n",
    "vv_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for vh in vh_paths:\n",
    "    vh.rename(vh_dir/vh.name)\n",
    "for vv in vv_paths:\n",
    "    vv.rename(vv_dir/vv.name)\n",
    "    \n",
    "if hyp3:\n",
    "    for d in rtc_dirs:\n",
    "        shutil.rmtree(d)\n",
    "else:\n",
    "    to_delete = [p for p in data_path.glob('*') if p.is_file()]\n",
    "    for p in to_delete:\n",
    "        p.unlink()\n",
    "\n",
    "vh_paths = sorted(list(vh_dir.glob('*VH*.tif')))\n",
    "vv_paths = sorted(list(vv_dir.glob('*VV*.tif')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a DataFrame containing file paths, acquisition dates, and EPSGs:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "current = Path(\"..\").resolve()\n",
    "sys.path.append(str(current))\n",
    "import util.util as util\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'file': vh_paths + vv_paths,\n",
    "    'data_type': ['VH_RTC' if 'VH' in str(pth) else 'VV_RTC' for pth in vh_paths + vv_paths],\n",
    "    'SAR_acquisition_dt': util.get_dates(vh_paths) + util.get_dates(vv_paths),\n",
    "    'EPSG': [util.get_epsg(p) for p in vh_paths + vv_paths]\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fix multiple UTM Zone-related issues\n",
    "\n",
    "Fix multiple UTM Zone-related issues should they exist in your data set. If multiple UTM zones are found, the following code cells will identify the predominant UTM zone and reproject the rest into that zone.\n",
    "\n",
    "**If the data fall into multiple EPSGs, identify the most heavily represented EPSG:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if df['EPSG'].nunique() > 1:\n",
    "    predominant_epsg = df['EPSG'].mode()[0]\n",
    "    print(f\"Predominant EPSG: {predominant_epsg}\")\n",
    "else: \n",
    "    predominant_epsg = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reproject tiffs to the predominate UTM:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osgeo import gdal\n",
    "gdal.UseExceptions()\n",
    "\n",
    "if predominant_epsg:\n",
    "    to_reproject = df.loc[df['EPSG'] != predominant_epsg, 'file']\n",
    "    for col in to_reproject.items():\n",
    "        geotiff_path = str(col[1])\n",
    "        gdal.Warp(geotiff_path, geotiff_path, srcSRS=f'EPSG:{util.get_epsg(geotiff_path)}', dstSRS=f'EPSG:{predominant_epsg}')\n",
    "        df.at[col[0], \"EPSG\"] = util.get_epsg(geotiff_path)\n",
    "    if df['EPSG'].nunique() > 1:\n",
    "        raise Exception('Expected a single EPSG for all VVs and DEMs')  \n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Merge multiple frames from the same date.\n",
    "\n",
    "If your AOI covers multiple frames or bursts, you will notice duplicate acquisition dates for multiple RTCs. We will merge these RTCs by date."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a dataFrame containing space-separated strings of paths to merge:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SAR_acquisition_date'] = df['SAR_acquisition_dt'].dt.date\n",
    "merge_df = df.groupby(['data_type', 'SAR_acquisition_date']).filter(lambda x: len(x) > 1)\n",
    "merge_df = merge_df.groupby(['data_type', 'SAR_acquisition_date'])['file'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "merge_df.columns = ['data_type', 'SAR_acquisition_date', 'path_merge_string']\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge all the frames for each date and delete the original tiffs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in merge_df.iterrows():\n",
    "    date_str = row['SAR_acquisition_date'].strftime('%Y%m%d')\n",
    "    og_path_0 = Path(row['path_merge_string'].split(' ')[0])\n",
    "    output_path = og_path_0.parent / f\"merged_{row['data_type']}_{date_str}.tif\"\n",
    "    cmd = f\"gdal_merge.py -o {output_path} {row['path_merge_string']}\"\n",
    "    print(cmd)\n",
    "    !$cmd\n",
    "    for p in row['path_merge_string'].split(' '):\n",
    "        Path(p).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Prepare_HydroSAR_RTC_Stack.ipynb - Version 1.0.0 - May 2024*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrosar [conda env:.local-hydrosar]",
   "language": "python",
   "name": "conda-env-.local-hydrosar-hydrosar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
