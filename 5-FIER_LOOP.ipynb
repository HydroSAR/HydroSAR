{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "098c263f-520d-4fc4-b0d3-0ea03991b5b3",
   "metadata": {},
   "source": [
    "<img src=\"https://radar.community.uaf.edu/wp-content/uploads/sites/667/2021/03/HydroSARbanner.jpg\" width=\"100%\" />\n",
    "\n",
    "<br>\n",
    "<font size=\"6\"> <b>FIER Daily Flood Forecasting Code</b><img style=\"padding: 7px\" src=\"https://radar.community.uaf.edu/wp-content/uploads/sites/667/2021/03/UAFLogo_A_647.png\" width=\"170\" align=\"right\"/></font>\n",
    "\n",
    "<br>\n",
    "<font size=\"4\"> <b> Franz J Meyer, University of Alaska Fairbanks</b> <br>\n",
    "</font>\n",
    "\n",
    "This notebooks is developing an algorithm to generate daily flood inundation predictions using time series of Sentinel-1 RTC data and GEOGLoWs river runoff forecasts. \n",
    "    \n",
    "The workflow utilizes information available in the fierpy <a href=\"https://github.com/SERVIR/fierpy\">fierpy</a> GitHub repository.\n",
    "<hr>\n",
    "\n",
    "\n",
    "\n",
    "TO DO: mode selection before rotation, change NN modes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50250be6-daef-416c-b9fb-770982ed6457",
   "metadata": {},
   "source": [
    "# Load Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06072e98-89d8-45b9-9911-b85403ac676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from ipyfilechooser import FileChooser\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import fierpy\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from ipyfilechooser import FileChooser\n",
    "import re\n",
    "from fier_local import reof as freof\n",
    "from fier_local import sel_best_fit\n",
    "import opensarlab_lib as asfn\n",
    "from osgeo import gdal, osr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "import ipywidgets as widgets\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import KFold\n",
    "import sklearn.metrics as metrics\n",
    "import warnings\n",
    "import sys\n",
    "import os\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "#%%capture\n",
    "from pathlib import Path\n",
    "import json # for loads\n",
    "import shutil\n",
    "from IPython.display import Markdown\n",
    "from IPython.display import display\n",
    "%matplotlib widget\n",
    "from ipyfilechooser import FileChooser\n",
    "import matplotlib.pyplot as plt \n",
    "plt.rcParams.update({'font.size': 12})\n",
    "import opensarlab_lib as asfn\n",
    "asfn.jupytertheme_matplotlib_format()\n",
    "from keras import backend as K\n",
    "from typing import Tuple\n",
    "import matplotlib.animation as animation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c819aa-7f62-46fa-a965-735d04f3701c",
   "metadata": {},
   "source": [
    "**Some functions to deal with geotiffs and arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3ae750-ab1c-4cbd-8f54-f27946b12889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function gets the dates of each file from their filename\n",
    "def get_dates(dir_path, prefix):\n",
    "    dates = []\n",
    "    pths = list(dir_path.glob(f'{prefix}.tif*'))\n",
    "\n",
    "    for p in pths:\n",
    "        date_regex = '\\d{8}'\n",
    "        date = re.search(date_regex, str(p))\n",
    "        if date:\n",
    "            dates.append(date.group(0))\n",
    "    return dates\n",
    "\n",
    "# Normalize the datasets\n",
    "# Define the normalization function\n",
    "def normalize(arr):\n",
    "    # Calculate the maximum value in the slice\n",
    "    max_val = np.max(arr)\n",
    "    min_val = np.min(arr)\n",
    "    # Normalize the slice by dividing each element by the maximum value\n",
    "    normalized_arr = (arr - min_val)/(max_val - min_val)\n",
    "    return normalized_arr\n",
    "\n",
    "# Append all entries of the dataset dictionnary. Used for when we skip SAR VH and SAR VH & VV\n",
    "def append_all(dic, name, to_append):\n",
    "        for key1, nested_level1 in dic.items():\n",
    "            for key2, nested_level2 in nested_level1.items():\n",
    "                nested_level2[f'{name}'].append(to_append)  \n",
    "                \n",
    "                \n",
    "# Used to grab the coordinates of the AOI defined by each geotiff               \n",
    "def get_coordinates_from_geotiff_bbox(geotiff_path):\n",
    "    dataset = gdal.Open(geotiff_path)\n",
    "\n",
    "    # Get the GeoTransform to convert pixel coordinates to geographical coordinates\n",
    "    transform = dataset.GetGeoTransform()\n",
    "\n",
    "    # Get the raster size (number of rows and columns)\n",
    "    cols = dataset.RasterXSize\n",
    "    rows = dataset.RasterYSize\n",
    "\n",
    "    # Get the four corner points in pixel coordinates\n",
    "    corners = [(0, 0), (cols, 0), (cols, rows), (0, rows)]\n",
    "\n",
    "    # Calculate the longitude (X) and latitude (Y) of each corner\n",
    "    corner_coordinates = []\n",
    "    for corner in corners:\n",
    "        lon = transform[0] + corner[0] * transform[1] + corner[1] * transform[2]\n",
    "        lat = transform[3] + corner[0] * transform[4] + corner[1] * transform[5]\n",
    "\n",
    "        # Get the spatial reference system of the dataset\n",
    "        srs = osr.SpatialReference()\n",
    "        srs.ImportFromWkt(dataset.GetProjection())\n",
    "\n",
    "        # Create a coordinate transformation object\n",
    "        target_srs = osr.SpatialReference()\n",
    "        target_srs.ImportFromEPSG(4326)  # EPSG code for WGS84\n",
    "        transform_obj = osr.CoordinateTransformation(srs, target_srs)\n",
    "\n",
    "        # Transform the coordinates to WGS84 (latitude and longitude)\n",
    "        lon, lat, _ = transform_obj.TransformPoint(lon, lat)\n",
    "\n",
    "        corner_coordinates.append((lat, lon))\n",
    "        \n",
    "    corner_coordinates = np.array(corner_coordinates)\n",
    "    boundaries = np.array([[np.min(corner_coordinates[:,1]), np.min(corner_coordinates[:,0])],\n",
    "                           [np.max(corner_coordinates[:,1]), np.max(corner_coordinates[:,0])]])\n",
    "\n",
    "    return boundaries\n",
    "\n",
    "\n",
    "# Used to grab the centerpoint coordinates of a geotiff\n",
    "def get_centerpoint_coordinates(tif_file):\n",
    "    dataset = gdal.Open(str(tif_file))\n",
    "    \n",
    "    # Get the geospatial transform\n",
    "    geotransform = dataset.GetGeoTransform()\n",
    "    \n",
    "    # Get the image size\n",
    "    width = dataset.RasterXSize\n",
    "    height = dataset.RasterYSize\n",
    "    \n",
    "    # Calculate the center pixel coordinates\n",
    "    center_x = geotransform[0] + (geotransform[1] * width + geotransform[2]) / 2\n",
    "    center_y = geotransform[3] + (geotransform[4] * height + geotransform[5]) / 2\n",
    "    \n",
    "    # Create a spatial reference object for EPSG:4326\n",
    "    src_srs = osr.SpatialReference()\n",
    "    src_srs.ImportFromEPSG(4326)\n",
    "    \n",
    "    # Create a spatial reference object for the TIF file\n",
    "    dataset_srs = osr.SpatialReference()\n",
    "    dataset_srs.ImportFromWkt(dataset.GetProjection())\n",
    "    \n",
    "    # Create a coordinate transformation object\n",
    "    coord_transform = osr.CoordinateTransformation(dataset_srs, src_srs)\n",
    "    \n",
    "    # Transform the center point coordinates to EPSG:4326\n",
    "    lon, lat, _ = coord_transform.TransformPoint(center_x, center_y)\n",
    "    \n",
    "    return lat, lon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0554f147-4e4b-43da-befb-43038197b7fe",
   "metadata": {},
   "source": [
    "**Function to create animated plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922cd15-b108-4508-ad62-f78fa2410810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that plots one image at a time\n",
    "def update_plot(frame, dataset, type_file):\n",
    "    # Extract the 2D slice corresponding to the current time step\n",
    "    current_slice = dataset[frame, :, :]\n",
    "    \n",
    "    # Clear the previous plot\n",
    "    plt.clf()\n",
    "    \n",
    "    # Depending on if your input iw water mask or RTC, the colormap changes\n",
    "    if type_file == 'Water_Masks':\n",
    "        plt.imshow(current_slice, cmap='Blues', interpolation=\"none\", vmin=0, vmax=1.2)\n",
    "    else:\n",
    "        # Plot the 2D slice\n",
    "        plt.imshow(current_slice, cmap='viridis', vmin = dataset.mean() - dataset.std(), vmax = dataset.mean() + dataset.std())\n",
    "    \n",
    "    # Add a title and other plot decorations (optional)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Add the date as a subtitle\n",
    "    current_date = str(dataset.time[frame].values)[:10]  # Assuming time is in datetime format\n",
    "    plt.text(0.5, 1.01, f'Date: {current_date}', transform=plt.gca().transAxes,\n",
    "             ha='center', va='bottom', fontsize=10)\n",
    " \n",
    "\n",
    "# Function to create the animated gifs\n",
    "def animate(dataset, fps, name, type_file):\n",
    "    \n",
    "    if os.path.exists(pathfig/f\"{name}.gif\"):\n",
    "        print(\"GIF already exists.\")\n",
    "    else:\n",
    "        # Create the animation\n",
    "        fig = plt.figure()\n",
    "        num_frames = da.shape[0]-1 # Because of Python's indexing and HTML indexing difference\n",
    "        ani = animation.FuncAnimation(fig, update_plot, frames=num_frames, fargs=(dataset,type_file,), blit=False)\n",
    "\n",
    "        # To show the animation in Jupyter Notebook (optional)\n",
    "\n",
    "        from IPython.display import HTML\n",
    "        HTML(ani.to_jshtml())\n",
    "\n",
    "        # Save the animation as a GIF\n",
    "        ani.save(pathfig/f\"{name}.gif\", writer='pillow', fps=fps)\n",
    "        plt.close(ani._fig)\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb8881a-cda4-47ad-ac7d-d6bfc1cf07a5",
   "metadata": {},
   "source": [
    "**Function to calculate the fits between data, discharge and precipitations, with a Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da99bc9-24ff-4ca9-b01d-b2a28083093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_fit_nn(var, reof_ds, dataset, models_neural):\n",
    "    \n",
    "    # Custom loss function\n",
    "    def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) \n",
    "    \n",
    "    # Get rid of the annoying warning that we should use datasets for optimized keras operations\n",
    "    tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "\n",
    "    # Create a list that will host the best models\n",
    "    models = []\n",
    "\n",
    "    # Define the neural network architecture. We are using ReLu according to Chang et al. (2023), with a linear output layer because it reduces the outliers and stabilized the fit.\n",
    "    def create_model():\n",
    "        model = keras.Sequential()\n",
    "        model.add(keras.layers.Dense(units = 64, activation = 'relu', input_shape=(1,)))\n",
    "        model.add(keras.layers.Dense(units = 64, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(units = 64, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(units = 64, activation = 'relu'))\n",
    "        model.add(keras.layers.Dense(units = 1, activation = 'linear'))\n",
    "        model.compile(loss=root_mean_squared_error, optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    # Create a dictionnary to store statistics of each loop\n",
    "    fit_dict = dict()\n",
    "    dict_keys = ['fit_r2','pred_r','pred_rmse']\n",
    "\n",
    "    # Generate sample data\n",
    "    hindcast_var = (var-var.min())/(var.max()-var.min())\n",
    "    \n",
    "    # Reconstructed dataset \n",
    "    recon_da = np.zeros((dataset.shape))\n",
    "\n",
    "    # Loop through temporal modes to determine the regression between them and variable\n",
    "    for mode in reof_ds.mode.values:\n",
    "\n",
    "        hindcast_rtcp = (\n",
    "            reof_ds.temporal_modes[:,mode-1] - reof_ds.temporal_modes[:,mode-1].min())/(reof_ds.temporal_modes[:,mode-1].max() - reof_ds.temporal_modes[:,mode-1].min())\n",
    "\n",
    "        # Set up K-fold cross-validation\n",
    "        k = 3\n",
    "        kf = KFold(n_splits=k)\n",
    "\n",
    "        # Perform K-fold cross-validation and evaluate the models\n",
    "        best_model = None\n",
    "        best_score = np.inf\n",
    "\n",
    "        for train_index, val_index in kf.split(hindcast_var):\n",
    "            X_train, X_test = hindcast_var[train_index], hindcast_var[val_index]\n",
    "            y_train, y_test = hindcast_rtcp[train_index], hindcast_rtcp[val_index]\n",
    "\n",
    "            model = create_model()\n",
    "            model.fit(X_train, y_train, epochs=100, verbose=0)\n",
    "            y_test_pred = model.predict(X_test)\n",
    "\n",
    "            rmse = metrics.mean_squared_error(y_test, y_test_pred,squared=False)\n",
    "\n",
    "            if rmse < best_score:\n",
    "                best_score = rmse\n",
    "                best_model = model\n",
    "\n",
    "        # Train the best model on the entire dataset\n",
    "        best_model.fit(hindcast_var, hindcast_rtcp, epochs=100, batch_size=10, verbose = 0)\n",
    "        \n",
    "        # Append the model in the list\n",
    "        models.append(best_model)\n",
    "\n",
    "        # Make predictions using the best model\n",
    "        hindcast_rtcp_pred = best_model.predict(hindcast_var, verbose = 0)\n",
    "\n",
    "        # Convert to dataarray\n",
    "        hindcast_rtcp_pred_dataarray = var.copy()\n",
    "        hindcast_rtcp_pred_dataarray.values = hindcast_rtcp_pred[:, 0]\n",
    "\n",
    "        # Add to the reconstructed dataset\n",
    "        recon_da += hindcast_rtcp_pred_dataarray * reof_ds.spatial_modes[:,:,mode-1]\n",
    "\n",
    "    # Calculate distance between dataset and modeled dataset\n",
    "    rmse_dataset = np.sqrt(np.mean((normalize(recon_da) - normalize(dataset)) ** 2)).values\n",
    "    \n",
    "    # Restore standard output\n",
    "    #sys.stdout = sys.__stdout__\n",
    "\n",
    "    return rmse_dataset, models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d745e8-feb6-4aef-815f-7776a1a0aa0a",
   "metadata": {},
   "source": [
    "**Function to synthesize the forecast based on Neural Network Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8596180d-5a0a-40ba-9efa-d0492132b11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_neural(var, model, reof_ds):\n",
    "    \n",
    "    # Create the variable filled with zeros\n",
    "    zeros_variable = np.zeros((var.time.shape[0], reof_ds.lat.shape[0], reof_ds.lon.shape[0]))\n",
    "\n",
    "    # Create the new DataArray with zeros_variable\n",
    "    da_slice = xr.DataArray(\n",
    "        zeros_variable,\n",
    "        coords={'time': var.time, 'lat': reof_ds.lat, 'lon': reof_ds.lon},\n",
    "        dims=['time', 'lat', 'lon']\n",
    "    )\n",
    "\n",
    "    # Loop over the modes\n",
    "    for m in range(len(model)):\n",
    "        \n",
    "        # Make predictions using the best model\n",
    "        forecast_rtcp = model[m].predict(var.values)\n",
    "        \n",
    "        # Convert to dataarray\n",
    "        forecast_rtcp_dataarray = var.copy()\n",
    "        forecast_rtcp_dataarray.values = forecast_rtcp[:, 0]\n",
    "\n",
    "        # Add to the reconstructed dataset\n",
    "        da_slice.values += forecast_rtcp_dataarray * reof_ds.spatial_modes[:,:,m]\n",
    "        \n",
    "        \n",
    "    return da_slice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a449b-6fee-4827-9daf-9da885a74331",
   "metadata": {},
   "source": [
    "#### **Main Function that is used to calculate the best fits between all the inputs and parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b4462-8199-452d-b7ec-4ff0f4cb70a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def looper(mm, type_file, pola, start_ind, metricranking, vv, fitting=False, plotting=False):\n",
    "    \n",
    "    # Load the appropriate files\n",
    "    if type_file == 'SAR':\n",
    "        folder = 'RTC_GAMMA/'\n",
    "        prefix = f'*{pola}'\n",
    "    else:\n",
    "        folder = 'Water_Masks/'\n",
    "        prefix = '*combined'\n",
    "\n",
    "    # Gather names of files corresponding to the file type and polarization we want\n",
    "    tiff_dir = Path(fc.selected)/folder\n",
    "    tiffs = list(tiff_dir.glob(f'{prefix}.tif*'))\n",
    "    \n",
    "    # Gather the date of each file\n",
    "    times = get_dates(tiff_dir, prefix)\n",
    "    times.sort()\n",
    "    times = pd.DatetimeIndex(times)\n",
    "    times.name = \"time\"\n",
    "    \n",
    "    # Create the dataset gathering the input images\n",
    "    da = xr.concat([rxr.open_rasterio(f) for f in tiffs[:stop_ind]], dim=times[:stop_ind])\n",
    "\n",
    "    # delete the extra data variable 'band'\n",
    "    da = da.sel(band=1, drop=True)\n",
    "    # rename autogenerated x,y as lon,lat \n",
    "    da = da.rename({'x': 'lon', 'y': 'lat'}).fillna(0)\n",
    "    \n",
    "    # Calculate the REOF of the dataset\n",
    "    print('Starting REOF')\n",
    "    reof_ds = (freof(da[:start_ind])).fillna(0)\n",
    "    print('Finished REOF')\n",
    "    \n",
    "    # Compute the polynomial and neural network fits\n",
    "    if fitting == True:    \n",
    "        # Calculate the fits\n",
    "        for var in ['Q','ERA']:\n",
    "\n",
    "            try:\n",
    "                #### ---- Polynomial ---- ####\n",
    "                # Calculate the fits of different polynomials concerning each main mode\n",
    "                fits = fierpy.find_fits(reof_ds,mm[var]['Selected'],da[:start_ind])\n",
    "                # Grab the best fitting mode and coefficients\n",
    "                name,mode,coeffs = sel_best_fit(fits, metricranking[0], metricranking[1])\n",
    "\n",
    "                if metricranking[0] == 'r2':\n",
    "                    suffix = 'fit'\n",
    "                else:\n",
    "                    suffix = 'pred'\n",
    "\n",
    "                # Put score of this coeff in the score list\n",
    "                vv[var]['Polynomial']['Score'].append(fits[f\"{'_'.join(name.split('_')[:2])}_{suffix}_{metricranking[0]}\"])\n",
    "                vv[var]['Polynomial']['Coeffs'].append(coeffs)\n",
    "                vv[var]['Polynomial']['Modes'].append(mode)\n",
    "\n",
    "            except:\n",
    "                print('Polynomial fit failed')\n",
    "                # If the polynomial failed, we automatically score extremely bad for discard\n",
    "                if metricranking[1] == 'max':\n",
    "                    vv[var]['Polynomial']['Score'].append(1e-9)\n",
    "                else:\n",
    "                    vv[var]['Polynomial']['Score'].append(1e9)\n",
    "                vv[var]['Polynomial']['Coeffs'].append(0)\n",
    "                vv[var]['Polynomial']['Modes'].append(0)                  \n",
    "\n",
    "\n",
    "\n",
    "            #### ---- Neural Network Regression ---- ####\n",
    "            \n",
    "            rmse, models = find_best_fit_nn(mm[var]['Selected'], reof_ds, da[:start_ind], vv[var]['Neural']['Models'])\n",
    "            vv[var]['Neural']['RMSE'].append(rmse)\n",
    "            vv[var]['Neural']['Models'].append(models)\n",
    "    \n",
    "    # Create gif showing the input\n",
    "    if plotting == True:\n",
    "        animate(da[:start_ind], 4, f\"Input_{type_file}_{pola}\", type_file)\n",
    "    \n",
    "    return reof_ds, da, vv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3b179b-f493-4957-92bf-f57ace86a1e3",
   "metadata": {},
   "source": [
    "**Functions to calculate flood masks based on Notebook 4: Water Mask from Iterative Thresholding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb70ca-6180-443e-85f5-329a73ed48d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EMSeg_opt(image, number_of_classes):\n",
    "    image_copy = image.copy()\n",
    "    image_copy2 = np.ma.filled(image.astype(float), np.nan) # needed for valid posterior_lookup keys\n",
    "    image = image.flatten()\n",
    "    minimum = np.amin(image)\n",
    "    image = image - minimum + 1\n",
    "    maximum = np.amax(image)\n",
    "\n",
    "    size = image.size\n",
    "    histogram = make_histogram(image)\n",
    "    nonzero_indices = np.nonzero(histogram)[0]\n",
    "    histogram = histogram[nonzero_indices]\n",
    "    histogram = histogram.flatten()\n",
    "    class_means = (\n",
    "            (np.arange(number_of_classes) + 1) * maximum /\n",
    "            (number_of_classes + 1)\n",
    "    )\n",
    "    class_variances = np.ones((number_of_classes)) * maximum\n",
    "    class_proportions = np.ones((number_of_classes)) * 1 / number_of_classes\n",
    "    sml = np.mean(np.diff(nonzero_indices)) / 1000\n",
    "    iteration = 0\n",
    "    while(True):\n",
    "        class_likelihood = make_distribution(\n",
    "            class_means, class_variances, class_proportions, nonzero_indices\n",
    "        )\n",
    "        sum_likelihood = np.sum(class_likelihood, 1) + np.finfo(\n",
    "                class_likelihood[0][0]).eps\n",
    "        log_likelihood = np.sum(histogram * np.log(sum_likelihood))\n",
    "        for j in range(0, number_of_classes):\n",
    "            class_posterior_probability = (\n",
    "                histogram * class_likelihood[:,j] / sum_likelihood\n",
    "            )\n",
    "            class_proportions[j] = np.sum(class_posterior_probability)\n",
    "            class_means[j] = (\n",
    "                np.sum(nonzero_indices * class_posterior_probability)\n",
    "                    / class_proportions[j]\n",
    "            )\n",
    "            vr = (nonzero_indices - class_means[j])\n",
    "            class_variances[j] = (\n",
    "                np.sum(vr *vr * class_posterior_probability)\n",
    "                    / class_proportions[j] +sml\n",
    "            )\n",
    "            del class_posterior_probability, vr\n",
    "        class_proportions = class_proportions + 1e-3\n",
    "        class_proportions = class_proportions / np.sum(class_proportions)\n",
    "        class_likelihood = make_distribution(\n",
    "            class_means, class_variances, class_proportions, nonzero_indices\n",
    "        )\n",
    "        sum_likelihood = np.sum(class_likelihood, 1) + np.finfo(\n",
    "                class_likelihood[0,0]).eps\n",
    "        del class_likelihood\n",
    "        new_log_likelihood = np.sum(histogram * np.log(sum_likelihood))\n",
    "        del sum_likelihood\n",
    "        if((new_log_likelihood - log_likelihood) < 0.000001):\n",
    "            break\n",
    "        iteration = iteration + 1\n",
    "    del log_likelihood, new_log_likelihood\n",
    "    class_means = class_means + minimum - 1\n",
    "    s = image_copy.shape\n",
    "    posterior = np.zeros((s[0], s[1], number_of_classes))\n",
    "    posterior_lookup = dict()\n",
    "    for i in range(0, s[0]):\n",
    "        for j in range(0, s[1]):\n",
    "            pixel_val = image_copy2[i,j] \n",
    "            if pixel_val in posterior_lookup:\n",
    "                for n in range(0, number_of_classes): \n",
    "                    posterior[i,j,n] = posterior_lookup[pixel_val][n]\n",
    "            else:\n",
    "                posterior_lookup.update({pixel_val: [0]*number_of_classes})\n",
    "                for n in range(0, number_of_classes): \n",
    "                    x = make_distribution(\n",
    "                        class_means[n], class_variances[n], class_proportions[n],\n",
    "                        image_copy[i,j]\n",
    "                    )\n",
    "                    posterior[i,j,n] = x * class_proportions[n]\n",
    "                    posterior_lookup[pixel_val][n] = posterior[i,j,n]\n",
    "    return posterior, class_means, class_variances, class_proportions\n",
    "\n",
    "def make_histogram(image):\n",
    "    image = image.flatten()\n",
    "    indices = np.nonzero(np.isnan(image))\n",
    "    image[indices] = 0\n",
    "    indices = np.nonzero(np.isinf(image))\n",
    "    image[indices] = 0\n",
    "    del indices\n",
    "    size = image.size\n",
    "    maximum = int(np.ceil(np.amax(image)) + 1)\n",
    "    #maximum = (np.ceil(np.amax(image)) + 1)\n",
    "    histogram = np.zeros((1, maximum))\n",
    "    for i in range(0,size):\n",
    "        #floor_value = int(np.floor(image[i]))\n",
    "        floor_value = np.floor(image[i]).astype(np.uint8)\n",
    "        #floor_value = (np.floor(image[i]))\n",
    "        if floor_value > 0 and floor_value < maximum - 1:\n",
    "            temp1 = image[i] - floor_value\n",
    "            temp2 = 1 - temp1\n",
    "            histogram[0,floor_value] = histogram[0,floor_value] + temp1\n",
    "            histogram[0,floor_value - 1] = histogram[0,floor_value - 1] + temp2\n",
    "    histogram = np.convolve(histogram[0], [1,2,3,2,1])\n",
    "    histogram = histogram[2:(histogram.size - 3)]\n",
    "    histogram = histogram / np.sum(histogram)\n",
    "    return histogram\n",
    "\n",
    "def make_distribution(m, v, g, x):\n",
    "    x = x.flatten()\n",
    "    m = m.flatten()\n",
    "    v = v.flatten()\n",
    "    g = g.flatten()\n",
    "    y = np.zeros((len(x), m.shape[0]))\n",
    "    for i in range(0,m.shape[0]):\n",
    "        d = x - m[i]\n",
    "        amp = g[i] / np.sqrt(2*np.pi*v[i])\n",
    "        y[:,i] = amp * np.exp(-0.5 * (d * d) / v[i])\n",
    "    return y\n",
    "\n",
    "def pad_image(image: np.ndarray, to: int) -> np.ndarray:\n",
    "    height, width = image.shape\n",
    "\n",
    "    n_rows, n_cols = get_tile_row_col_count(height, width, to)\n",
    "    new_height = n_rows * to\n",
    "    new_width = n_cols * to\n",
    "\n",
    "    padded = np.zeros((new_height, new_width))\n",
    "    padded[:image.shape[0], :image.shape[1]] = image\n",
    "    return padded\n",
    "\n",
    "#def tile_image(image: np.ndarray, width: int = 200, height: int = 200) -> np.ndarray:\n",
    "def tile_image(image: np.ndarray, width, height) -> np.ndarray:\n",
    "    _nrows, _ncols = image.shape\n",
    "    _strides = image.strides\n",
    "\n",
    "    nrows, _m = divmod(_nrows, height)\n",
    "    ncols, _n = divmod(_ncols, width)\n",
    "\n",
    "    assert _m == 0, \"Image must be evenly tileable. Please pad it first\"\n",
    "    assert _n == 0, \"Image must be evenly tileable. Please pad it first\"\n",
    "\n",
    "    return np.lib.stride_tricks.as_strided(\n",
    "        np.ravel(image),\n",
    "        shape=(nrows, ncols, height, width),\n",
    "        strides=(height * _strides[0], width * _strides[1], *_strides),\n",
    "        writeable=False\n",
    "    ).reshape(nrows * ncols, height, width)\n",
    "\n",
    "def gdal_get_geotransform(filename):\n",
    "    '''\n",
    "    [top left x, w-e pixel resolution, rotation, top left y, rotation, n-s pixel resolution]=gdal_get_geotransform('/path/to/file')\n",
    "    '''\n",
    "    filename = str(filename)\n",
    "    #http://stackoverflow.com/questions/2922532/obtain-latitude-and-longitude-from-a-geotiff-file\n",
    "    ds = gdal.Open(filename)\n",
    "    return ds.GetGeoTransform()\n",
    "\n",
    "def gdal_read(filename, ndtype=np.float64):\n",
    "    '''\n",
    "    z=readData('/path/to/file')\n",
    "    '''\n",
    "    filename = str(filename)\n",
    "    ds = gdal.Open(filename) \n",
    "    return np.array(ds.GetRasterBand(1).ReadAsArray()).astype(ndtype);\n",
    "\n",
    "def get_tile_row_col_count(height: int, width: int, tile_size: int) -> Tuple[int, int]:\n",
    "    return int(np.ceil(height / tile_size)), int(np.ceil(width / tile_size))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Main Water Mask function using an iterating thresholding method to recover areas flooded. VH = False means we don't have a SAR VH image (used for the forecasts). When True, it means we have a SAR VH image (for the hindcast)\n",
    "def flood_mask_iterative(img_array, handem, VH = False):\n",
    "\n",
    "\n",
    "    # define some variables you might want to change\n",
    "    precentile = 0.95        # Standard deviation percentile threshold for pivotal tile selection\n",
    "    tilesize = 100           # Setting tile size to use in thresholding algorithm\n",
    "    tilesize2 = 50\n",
    "    Hpick = 0.8              # Threshold for required fraction of valid HAND-EM pixels per tile\n",
    "    vv_corr = -15.0         # VV threshold to use if threshold calculation did not succeed\n",
    "    vh_corr = -23.0         # VH threshold to use if threshold calculation did not succeed\n",
    "\n",
    "    # Tile up HAND-EM data\n",
    "    handem_p = pad_image(handem, tilesize)\n",
    "    hand_tiles = tile_image(handem_p,width=tilesize,height=tilesize)\n",
    "    Hsum = np.sum(hand_tiles, axis=(1,2))\n",
    "    Hpercent = Hsum/(tilesize*tilesize)\n",
    "\n",
    "    # Now do adaptive threshold selection\n",
    "    vv_thresholds = np.array([])\n",
    "    vh_thresholds = np.array([])\n",
    "    floodarea = np.array([])\n",
    "    vh_thresholds_corr = np.array([])\n",
    "    vv_thresholds_corr = np.array([])\n",
    "\n",
    "    posterior_lookup = dict()\n",
    "\n",
    "    ##### ---------- VH THRESHOLD CALCULATION ----------- #####\n",
    "    \n",
    "    # If we are calculating the mask for the hindcast, we have a real VH image to use\n",
    "    if VH:\n",
    "        # Load the list of VH images\n",
    "        tiff_dir = Path(fc.selected)/'RTC_GAMMA/'\n",
    "        tiffs = list(tiff_dir.glob(f'*VH.tif*'))\n",
    "        # Load the image corresponding to the SAR VH hindcast\n",
    "        vh_array = rxr.open_rasterio(tiffs[stop_ind]).values[0]\n",
    "    else:   \n",
    "        # If we are calculating the mask for the forecast, we don't have a SAR VH so we use SAR VV\n",
    "        vh_array = img_array\n",
    "       \n",
    "    \n",
    "    vh_array = pad_image(vh_array, tilesize)\n",
    "\n",
    "    invalid_pixels = np.nonzero(vh_array == 0.0)\n",
    "    vh_tiles = tile_image(vh_array,width=tilesize,height=tilesize)\n",
    "    a = np.shape(vh_tiles)\n",
    "    vh_std = np.zeros(a[0])\n",
    "    vht_masked = np.ma.masked_where(vh_tiles==0, vh_tiles)\n",
    "    vh_picktiles = np.zeros_like(vh_tiles)\n",
    "    for k in range(a[0]):\n",
    "        vh_subtiles = tile_image(vht_masked[k,:,:],width=tilesize2,height=tilesize2)\n",
    "        vhst_mean = np.ma.mean(vh_subtiles, axis=(1,2))\n",
    "        vhst_std = np.ma.std(vhst_mean)\n",
    "        vh_std[k] = np.ma.std(vhst_mean)\n",
    "\n",
    "    # find tiles with largest standard deviations\n",
    "    vh_mean = np.ma.median(vht_masked, axis=(1,2))\n",
    "    x_vh = np.sort(vh_std/vh_mean)\n",
    "    xm_vh = np.sort(vh_mean)\n",
    "    #x_vh = np.sort(vh_std)            \n",
    "    y_vh = np.arange(1, x_vh.size+1) / x_vh.size\n",
    "    ym_vh = np.arange(1, xm_vh.size+1) / xm_vh.size\n",
    "\n",
    "    percentile2 = precentile\n",
    "    sort_index = 0\n",
    "    while np.size(sort_index) < 5:\n",
    "        threshold_index_vh = np.ma.min(np.where(y_vh>percentile2))\n",
    "        threshold_vh = x_vh[threshold_index_vh]\n",
    "        #sd_select_vh = np.nonzero(vh_std/vh_mean>threshold_vh)\n",
    "        s_select_vh = np.nonzero(vh_std/vh_mean>threshold_vh) \n",
    "        h_select_vh = np.nonzero(Hpercent > Hpick)               # Includes HAND-EM in selection\n",
    "        sd_select_vh = np.intersect1d(s_select_vh, h_select_vh)\n",
    "\n",
    "        # find tiles with mean values lower than the average mean \n",
    "        omean_vh = np.ma.median(vh_mean[h_select_vh[0][h_select_vh[0]<vh_mean.shape[0]]])\n",
    "        mean_select_vh = np.nonzero(vh_mean<omean_vh)\n",
    "\n",
    "        # Intersect tiles with large std with tiles that have small means\n",
    "        msdselect_vh = np.intersect1d(sd_select_vh, mean_select_vh)\n",
    "        sort_index = np.flipud(np.argsort(vh_std[msdselect_vh]))\n",
    "        percentile2 = percentile2 - 0.01\n",
    "    finalselect_vh = sort_index[0:5]\n",
    "\n",
    "\n",
    "    # find local thresholds for 5 \"best\" tiles in the image\n",
    "    l_thresh_vh = np.zeros(5)\n",
    "    EMthresh_vh = np.zeros(5)\n",
    "    temp = np.ma.masked_where(vh_array==0, vh_array)\n",
    "    dbvh = np.ma.log10(temp)+30\n",
    "    scaling = 256/(np.mean(dbvh) + 3*np.std(dbvh))\n",
    "    #scaling = 256/(np.mean(vh_array) + 3*np.std(vh_array))\n",
    "    dbtile = np.ma.log10(vht_masked)+30\n",
    "    for k in range(5):\n",
    "        test = dbtile[msdselect_vh[finalselect_vh[k]]] * scaling\n",
    "        #test = vht_masked[msdselect_vh[finalselect_vh[k]]] * scaling\n",
    "        A = np.around(test)\n",
    "        A = A.astype(int)\n",
    "        #t_thresh = Kittler(A)\n",
    "        [posterior, cm, cv, cp] = EMSeg_opt(A, 3)\n",
    "        sorti = np.argsort(cm)\n",
    "        cms = cm[sorti]\n",
    "        cvs = cv[sorti]\n",
    "        cps = cp[sorti]\n",
    "        xvec = np.arange(cms[0],cms[1],step=.05)\n",
    "        x1 = make_distribution(cms[0], cvs[0], cps[0], xvec)\n",
    "        x2 = make_distribution(cms[1], cvs[1], cps[1], xvec)\n",
    "        dx = np.abs(x1 - x2)\n",
    "        diff1 = posterior[:,:,0] - posterior[:,:,1]\n",
    "        t_ind = np.argmin(dx)\n",
    "        EMthresh_vh[k] = xvec[t_ind]/scaling\n",
    "\n",
    "        #l_thresh_vh[k] = t_thresh / scaling\n",
    "\n",
    "\n",
    "        # Mark Tiles used for Threshold Estimation\n",
    "        vh_picktiles[msdselect_vh[finalselect_vh[k]],:,:]= np.ones_like(vh_tiles[msdselect_vh[finalselect_vh[k]],:,:])\n",
    "\n",
    "    # Calculate best threshold for VV and VH as the mean of the 5 thresholds calculated in the previous section \n",
    "    #m_thresh_vh = np.median(l_thresh_vh)\n",
    "    #print(EMthresh_vh-30)\n",
    "    EMts = np.sort(EMthresh_vh)\n",
    "    #m_thresh_vh = np.median(EMthresh_vh)\n",
    "    m_thresh_vh = np.median(EMts[0:4])\n",
    "\n",
    "    # Derive flood mask using the best threshold\n",
    "    maskedarray = np.ma.masked_where(dbvh==0, dbvh)\n",
    "\n",
    "    #maskedarray = np.ma.masked_where(vh_array==0, vh_array)\n",
    "    if m_thresh_vh < (vh_corr/10.0+30):\n",
    "        change_mag_mask_vh = maskedarray < m_thresh_vh\n",
    "        vh_thresholds_corr = np.append(vh_thresholds_corr, 10.0*(m_thresh_vh-30)) \n",
    "        #change_mag_mask_vv = np.ma.masked_where(vv_array==0, vv_array) < m_thresh_vv\n",
    "    else:\n",
    "        change_mag_mask_vh = maskedarray < (vh_corr/10.0+30)\n",
    "        vh_thresholds_corr = np.append(vh_thresholds_corr, vh_corr) \n",
    "    # change_mag_mask_vh = vh_array < m_thresh_vh\n",
    "\n",
    "    # Create Binary masks showing flooded pixels as \"1\"s\n",
    "    sel = np.ones_like(vh_array)\n",
    "    flood_vh = np.zeros_like(vh_array)\n",
    "    flood_vh[change_mag_mask_vh] = sel[change_mag_mask_vh]\n",
    "    np.putmask(flood_vh,vh_array==0 , 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ##### ---------- VV MASK CALCULATION ------------ #####\n",
    "\n",
    "\n",
    "    original_shape = img_array.shape\n",
    "    n_rows, n_cols = get_tile_row_col_count(*original_shape, tile_size=tilesize)\n",
    "\n",
    "    vv_array = pad_image(img_array, tilesize)\n",
    "    invalid_pixels = np.nonzero(vv_array == 0.0)\n",
    "    vv_tiles = tile_image(vv_array,width=tilesize,height=tilesize)\n",
    "    a = np.shape(vv_tiles)\n",
    "    vv_std = np.zeros(a[0])\n",
    "    vvt_masked = np.ma.masked_where(vv_tiles==0, vv_tiles)\n",
    "    vv_picktiles = np.zeros_like(vv_tiles)\n",
    "    for k in range(a[0]):\n",
    "        vv_subtiles = tile_image(vvt_masked[k,:,:],width=tilesize2,height=tilesize2)\n",
    "        vvst_mean = np.ma.mean(vv_subtiles, axis=(1,2))\n",
    "        vvst_std = np.ma.std(vvst_mean)\n",
    "        vv_std[k] = np.ma.std(vvst_mean) \n",
    "\n",
    "    # find tiles with largest standard deviations\n",
    "    vv_mean = np.ma.median(vvt_masked, axis=(1,2))\n",
    "    x_vv = np.sort(vv_std/vv_mean)\n",
    "    y_vv = np.arange(1, x_vv.size+1) / x_vv.size\n",
    "\n",
    "    percentile2 = precentile\n",
    "    sort_index = 0\n",
    "    while np.size(sort_index) < 5: \n",
    "        threshold_index_vv = np.ma.min(np.where(y_vv>percentile2))\n",
    "        threshold_vv = x_vv[threshold_index_vv]\n",
    "        #sd_select_vv = np.nonzero(vv_std/vv_mean>threshold_vv)\n",
    "        s_select_vv = np.nonzero(vv_std/vv_mean>threshold_vv) \n",
    "        h_select_vv = np.nonzero(Hpercent > Hpick)               # Includes HAND-EM in selection\n",
    "        sd_select_vv = np.intersect1d(s_select_vv, h_select_vv)\n",
    "\n",
    "        # find tiles with mean values lower than the average mean\n",
    "        omean_vv = np.ma.median(vv_mean[h_select_vv[0][h_select_vv[0]<vv_mean.shape[0]]])\n",
    "        mean_select_vv = np.nonzero(vv_mean<omean_vv)\n",
    "\n",
    "        # Intersect tiles with large std with tiles that have small means\n",
    "        msdselect_vv = np.intersect1d(sd_select_vv, mean_select_vv)\n",
    "        sort_index = np.flipud(np.argsort(vv_std[msdselect_vv]))\n",
    "        percentile2 = percentile2 - 0.01\n",
    "    finalselect_vv = sort_index[0:5]\n",
    "\n",
    "    # find local thresholds for 5 \"best\" tiles in the image\n",
    "    l_thresh_vv = np.zeros(5)\n",
    "    EMthresh_vv = np.zeros(5)\n",
    "    temp = np.ma.masked_where(vv_array==0, vv_array)\n",
    "    dbvv = np.ma.log10(temp)+30\n",
    "    scaling = 256/(np.mean(dbvv) + 3*np.std(dbvv))\n",
    "    #scaling = 256/(np.mean(vv_array) + 3*np.std(vv_array))\n",
    "    dbtile = np.ma.log10(vvt_masked)+30\n",
    "    for k in range(5):\n",
    "        test = dbtile[msdselect_vh[finalselect_vh[k]]] * scaling\n",
    "        #test = vvt_masked[msdselect_vv[finalselect_vv[k]]] * scaling\n",
    "        A = np.around(test)\n",
    "        A = A.astype(int)\n",
    "        #t_thresh = Kittler(A)\n",
    "        [posterior, cm, cv, cp] = EMSeg_opt(A, 3)\n",
    "        sorti = np.argsort(cm)\n",
    "        cms = cm[sorti]\n",
    "        cvs = cv[sorti]\n",
    "        cps = cp[sorti]\n",
    "        xvec = np.arange(cms[0],cms[1],step=.05)\n",
    "        x1 = make_distribution(cms[0], cvs[0], cps[0], xvec)\n",
    "        x2 = make_distribution(cms[1], cvs[1], cps[1], xvec)\n",
    "        dx = np.abs(x1 - x2)\n",
    "        diff1 = posterior[:,:,0] - posterior[:,:,1]\n",
    "        t_ind = np.argmin(dx)\n",
    "        EMthresh_vv[k] = xvec[t_ind]/scaling\n",
    "\n",
    "        #l_thresh_vv[k] = t_thresh / scaling\n",
    "        #dbtile = np.ma.log10(vvt_masked)+30\n",
    "\n",
    "        # Mark Tiles used for Threshold Estimation\n",
    "        vv_picktiles[msdselect_vh[finalselect_vh[k]],:,:]= np.ones_like(vv_tiles[msdselect_vh[finalselect_vh[k]],:,:])\n",
    "\n",
    "    # Calculate best threshold for VV and VH as the mean of the 5 thresholds calculated in the previous section \n",
    "    #m_thresh_vv = np.median(l_thresh_vv)\n",
    "    #print(EMthresh_vv-30)\n",
    "    EMts = np.sort(EMthresh_vv)\n",
    "    #m_thresh_vv = np.median(EMthresh_vv)\n",
    "    m_thresh_vv = np.median(EMts[0:4])\n",
    "\n",
    "    # Derive flood mask using the best threshold\n",
    "    if m_thresh_vv < (vv_corr/10.0+30):\n",
    "        change_mag_mask_vv = np.ma.masked_where(dbvv==0, dbvv) < m_thresh_vv\n",
    "        vv_thresholds_corr = np.append(vv_thresholds_corr, 10.0*(m_thresh_vv-30))\n",
    "        #change_mag_mask_vv = np.ma.masked_where(vv_array==0, vv_array) < m_thresh_vv\n",
    "    else:\n",
    "        change_mag_mask_vv = np.ma.masked_where(dbvv==0, dbvv) < (vv_corr/10.0+30)\n",
    "        vv_thresholds_corr = np.append(vv_thresholds_corr, vv_corr)\n",
    "\n",
    "    # Create Binary masks showing flooded pixels as \"1\"s\n",
    "    flood = np.zeros_like(vv_array)\n",
    "    sel = np.ones_like(vv_array)\n",
    "    flood[change_mag_mask_vv] = sel[change_mag_mask_vv]\n",
    "    np.putmask(flood,vv_array==0 , 0)\n",
    "\n",
    "    return flood\n",
    "\n",
    "\n",
    "# Function calling the water mask function. If we are using water masks as input then the water mask function is skipped\n",
    "def Z_Score_Iterative(vv, var, reg, handem):\n",
    "\n",
    "    # Pre-fill an empty array to receive the forecasted water masks\n",
    "    flood_forecast = np.zeros((vv[var][reg]['Forecast'].shape[0], pad_image(vv[var][reg]['Forecast'][0], 100).shape[0], pad_image(vv[var][reg]['Forecast'][0], 100).shape[1]))\n",
    "    \n",
    "    #### ---- If SAR VV ---- ####\n",
    "    if vv[var][reg]['Filetype'] == 'SAR' and vv[var][reg]['Polarization'] == 'VV':\n",
    "\n",
    "        # We iterate through the forecasts\n",
    "        for iter in range(vv[var][reg]['Forecast'].shape[0]):\n",
    "            flood_forecast[iter] = flood_mask_iterative(vv[var][reg]['Forecast'][iter], handem)\n",
    "            \n",
    "            print(f\"Mask {iter+1}/{vv[var][reg]['Forecast'].shape[0]}\")\n",
    "            \n",
    "            # For the last forecast, we can compare the mask with the hindcast\n",
    "            if iter == vv[var][reg]['Forecast'].shape[0]-1:\n",
    "                flood_hindcast = flood_mask_iterative(vv[var][reg]['Hindcast'][0], handem, VH = True)\n",
    "\n",
    "\n",
    "    #### ---- If not SAR VV, it's a water mask ---- ####\n",
    "    else:\n",
    "\n",
    "        # We round the forecast because the REOF does not give out binary data. We expect to represent most of the dataset's variance so rounding is expected to interpolate well\n",
    "        # We iterate through the forecasts\n",
    "        for iter in range(vv[var][reg]['Forecast'].shape[0]):\n",
    "            flood_forecast[iter] = np.round(vv[var][reg]['Forecast'][iter])\n",
    "            print(f\"Mask {iter+1}/{vv[var][reg]['Forecast'].shape[0]}\")\n",
    "        \n",
    "            # For the last forecast, we can compare the mask with the hindcast\n",
    "            if iter == vv[var][reg]['Forecast'].shape[0]-1:\n",
    "                flood_hindcast = vv[var][reg]['Hindcast'][0]\n",
    "\n",
    "\n",
    "    #### ---- SCORE ---- ####\n",
    "\n",
    "    # Calculate the indices for scoring: a = true positive, b = false positive, c = false negative, d = true negative\n",
    "    a = np.sum(np.logical_and(flood_forecast[-1] == 1, flood_hindcast == 1))\n",
    "    b = np.sum(np.logical_and(flood_forecast[-1] == 1, flood_hindcast == 0))\n",
    "    c = np.sum(np.logical_and(flood_forecast[-1] == 0, flood_hindcast == 1))\n",
    "    d = np.sum(np.logical_and(flood_forecast[-1] == 0, flood_hindcast == 0))\n",
    "\n",
    "    # Calculate the skills\n",
    "    overall_accuracy = ((a + b) / (a + b + c + d)) * 100\n",
    "    CSI = (a / (a + b + c)) * 100\n",
    "    precision = (a / (a + b)) * 100\n",
    "    recall = (a / (a + c)) * 100\n",
    "    \n",
    "\n",
    "    return overall_accuracy, CSI, precision, recall, flood_forecast, flood_hindcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad71c05e-2b63-4fed-bd89-e7f7f3282502",
   "metadata": {},
   "source": [
    "**Function to create z-score flood maps from forecast and RTCs, and calculate their correspondance according to DeVries (2020) (results not as promising as the water mask thresholding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e04bd98-956a-40d4-964c-c73bf834d6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(mv, vv, var, reg, nb_dry):\n",
    "    \n",
    "    # Define a few variables\n",
    "    filetype = vv[var][reg]['Filetype']\n",
    "    dataset = vv[var][reg]['Training_Data']\n",
    "    variable = mv[var]['Selected']\n",
    "    flood_forecast = np.zeros((vv[var][reg]['Forecast'].shape))\n",
    "    \n",
    "    # There is only 1 hindcast image: corresponding to the last forecast's day\n",
    "    hindcast_slice = vv[var][reg]['Hindcast']\n",
    "    \n",
    "    # If the input is already water mask, we can compare them directly\n",
    "    if vv[var][reg]['Filetype'] == 'Water_Mask':\n",
    "        \n",
    "        for iter in range(vv[var][reg]['Forecast'].shape[0]):\n",
    "            \n",
    "            # We round the forecast because the REOF does not give out binary data. We expect to represent most of the dataset's variance so rounding is expected to interpolate well\n",
    "            flood_forecast[iter] = np.round(vv[var][reg]['Forecast'][iter])\n",
    "\n",
    "            # We only compute the score for the last slice of the forecast, aka the only slice we can compare\n",
    "            if iter == vv[var][reg]['Forecast'].shape[0]-1:\n",
    "                \n",
    "                # If input is Water_Mask, hindcast = water_mask\n",
    "                flood_hindcast = hindcast_slice\n",
    "                # Calculate the indices for scoring: a = true positive, b = false positive, c = false negative, d = true negative\n",
    "                a = np.sum(np.logical_and(flood_forecast[iter] >= 0.44, flood_hindcast == 1))\n",
    "                b = np.sum(np.logical_and(flood_forecast[iter] >= 0.44, flood_hindcast == 0))\n",
    "                c = np.sum(np.logical_and(flood_forecast[iter] <= 0.44, flood_hindcast == 1))\n",
    "                d = np.sum(np.logical_and(flood_forecast[iter] <= 0.44, flood_hindcast == 0))\n",
    "                # Calculate the skills\n",
    "                overall_accuracy = ((a + b) / (a + b + c + d)) * 100\n",
    "                CSI = (a / (a + b + c)) * 100\n",
    "                precision = (a / (a + b)) * 100\n",
    "                recall = (a / (a + c)) * 100\n",
    "        \n",
    "    else: # If the input is SAR, then we have to calculate the z-score depending on the method from DeVries\n",
    "\n",
    "        # Create dataframe\n",
    "        df = pd.DataFrame({'time': times[:start_ind], 'var': variable})\n",
    "\n",
    "        # Extract the year and month from the time column\n",
    "        df['year'] = df['time'].dt.year\n",
    "        df['month'] = df['time'].dt.month\n",
    "\n",
    "        # Group the DataFrame by year and month to calculate the monthly q_sel average\n",
    "        monthly_avg = df.groupby(['year', 'month'])['var'].mean().reset_index()\n",
    "\n",
    "        # Group the DataFrame by year and count the number of unique months\n",
    "        month_counts = monthly_avg.groupby('year')['month'].nunique()\n",
    "\n",
    "        # Get the years with at least nb_dry unique months\n",
    "        valid_years = month_counts[month_counts >= nb_dry].index\n",
    "\n",
    "        # Filter the DataFrame to include only the valid years\n",
    "        filtered_df = df[df['year'].isin(valid_years)]\n",
    "\n",
    "        # Group the filtered DataFrame by year and month to calculate the final monthly q_sel average\n",
    "        monthly_avg_filtered = filtered_df.groupby(['year', 'month'])['var'].mean().reset_index()\n",
    "\n",
    "        # Get the four months with the lowest average q_sel for each year\n",
    "        top_months = monthly_avg_filtered.groupby('year')['var'].nsmallest(nb_dry).index.get_level_values(1)\n",
    "\n",
    "        # Filter the DataFrame again to include only the top months\n",
    "        filtered_df = filtered_df[filtered_df['month'].isin(top_months)]\n",
    "        \n",
    "        # Calculate the average and std of the baseline images of da that fall in the dry season indices\n",
    "        # We convert to dB follow DeVries (2020)\n",
    "        dataset2 = 10*np.log(dataset.where(dataset!=0, 1))\n",
    "        average = dataset2.isel(time=filtered_df.index).mean()\n",
    "        standard_deviation = dataset2.isel(time=filtered_df.index).std()\n",
    "\n",
    "        for iter in range(vv[var][reg]['Forecast'].shape[0]):\n",
    "            \n",
    "            # Attribute the value\n",
    "            forecast_slice = vv[var][reg]['Forecast'][iter]\n",
    "            \n",
    "            # Calculate the z_score\n",
    "            z_score_forecast = (10*np.log(forecast_slice.where(forecast_slice != 0, other=1e-9)) - average) /  standard_deviation\n",
    "            \n",
    "            # Create flood matrices, where 0 is no flood and 1 is flood. Threshold is at -3 for flood pixels, following DeVries (2020)\n",
    "            flood_forecast[iter, z_score_forecast < -3] = 1\n",
    "            \n",
    "            # We only compute the score for the last slice of the forecast, aka the only slice we can compare\n",
    "            if iter == vv[var][reg]['Forecast'].shape[0]-1:\n",
    "                \n",
    "                z_score_hindcast = (10*np.log(hindcast_slice.where(forecast_slice != 0, other=1e-9)) - average) /  standard_deviation\n",
    "                flood_hindcast = np.zeros((z_score_hindcast.shape))\n",
    "                flood_hindcast[z_score_hindcast < -3] = 1\n",
    "\n",
    "                # Calculate the indices for scoring: a = true positive, b = false positive, c = false negative, d = true negative\n",
    "                a = np.sum(np.logical_and(flood_forecast[iter] == 1, flood_hindcast == 1))\n",
    "                b = np.sum(np.logical_and(flood_forecast[iter] == 1, flood_hindcast == 0))\n",
    "                c = np.sum(np.logical_and(flood_forecast[iter] == 0, flood_hindcast == 1))\n",
    "                d = np.sum(np.logical_and(flood_forecast[iter] == 0, flood_hindcast == 0))\n",
    "\n",
    "                # Calculate the skills\n",
    "                overall_accuracy = ((a + b) / (a + b + c + d)) * 100\n",
    "                CSI = (a / (a + b + c)) * 100\n",
    "                precision = (a / (a + b)) * 100\n",
    "                recall = (a / (a + c)) * 100\n",
    "\n",
    "    return overall_accuracy, CSI, precision, recall, flood_forecast, flood_hindcast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c7b89-dd45-49df-9aec-c7c099163d8d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c25821-df72-4bb9-b032-fa04a942cffc",
   "metadata": {},
   "source": [
    "# Choose the folder of the area you want to work with (subfolder of \"Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622cc161-c42b-4562-a34c-a14e867a3c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = FileChooser(Path.cwd())\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dae8c8-a99b-4a28-9c12-ec60ead98506",
   "metadata": {},
   "source": [
    "#### **Instructions**\n",
    "\n",
    "**1 - Generate the flood percentage figure**  \n",
    "\n",
    "**2 - Select a date range (format 'YYYY-MM-DD', or index)**\n",
    "\n",
    "**2 - Choose the criteria on which you want to base the best fit selection (rmse, r2, r).**\n",
    "\n",
    "- Choose between \"rmse\", \"r\", \"r2\"\n",
    "- If you choose \"r2\", you have to write \"max\"\n",
    "- \"min\" otherwise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595b3657-4171-4a33-9ad0-252d9bfa7252",
   "metadata": {},
   "source": [
    "## **Variables Description**\n",
    "\n",
    "We divide the variables of this notebook in multiple categories: \n",
    "\n",
    "**Level 1:**\n",
    "- Variables to fit: Discharge and Precipitation ('Q' and 'ERA')\n",
    "- Variables Generated\n",
    "\n",
    "**Level 2:**\n",
    "Only for the variables generated.\n",
    "- Polynomial fit\n",
    "- Neural Network fit\n",
    "\n",
    "**Level 3:**\n",
    "- Reof\n",
    "- Training Data (stack of SAR or Water Mask images on which the fit is calculated, polarization VV, VH or both)\n",
    "- Filetype (SAR or Water Mask)\n",
    "- Polarization (VV, VH, VV & VH)\n",
    "- Coefficients (for polynomial fit only, coefficients of the best fitting polynomial)\n",
    "- Modes (for polynomial fit only, mode with the best fit to the variables to fit)\n",
    "- Models (for neural network only, model with the best variables fit)\n",
    "- Score (for polynomial only, to help decide which fit is the best)\n",
    "- RMSE (score for neural network to see which model is the best)\n",
    "- Reof_Forecast (reof of the images we forecasted)\n",
    "- Reof_Hindcast (reof of the images we hindcast to compare with forecast)\n",
    "- Hindcast (hindcast stack of images)\n",
    "- Forecast (forecast stack of images)\n",
    "- Best_Index (index of the best score)\n",
    "- Difference (difference of Reofs of forecast and hindcast)\n",
    "\n",
    "**Dictionnary of variables: vz**\n",
    "We gather our main variables in a dictionnary called vz. This dictionnary is organized based on the levels described above.\n",
    "\n",
    "- Level 1: 'Q' and 'ERA'\n",
    "- Level 2: 'Polynomial' and 'Neural'\n",
    "- Level 3: 'Reof', 'Training_Data', 'Filetype', 'Polarization',\n",
    "'Coeffs', 'Modes', 'Models', 'Score', 'RMSE', 'Reof_forecast',\n",
    "                'Reof_hindcast', 'Hindcast', 'Forecast', 'Best_Index', 'Difference',\n",
    "                'Water_Hindcast', 'Water_Forecast'\n",
    "\n",
    "\n",
    "For example: *vz['Q']['Neural']['Forecast']* will give us the dataarray gathering the flood forecast.\n",
    "\n",
    "*vz['ERA']['Polynomial']['Coeffs']* will give us the coefficients of the polynomial fitting the best the function f(precipitation) = temporal_modes\n",
    "\n",
    "But we can also find information related to the type of file used or the polarization (*vz['ERA']['Polynomial']['Filetype']* and *vz['ERA']['Polynomial']['Polarization']* respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03790731-12e8-484b-b963-e1a39c05297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of available dataset\n",
    "list_types = ['SAR', 'Water_Masks']\n",
    "# List of available polarizations\n",
    "list_polarization = ['VV', 'VH and VV'] #['VH', 'VH and VV', 'VV']\n",
    "\n",
    "# Create the folder for the figures\n",
    "pathfig = Path(fc.selected)/'Figures'\n",
    "pathfig.mkdir(exist_ok=True)\n",
    "\n",
    "# Create vz, the dictionnary to hold our many variables\n",
    "vz = {\n",
    "    key1: {\n",
    "        key2: {\n",
    "            sub_key: [] for sub_key in [\n",
    "                'Reof', 'Training_Data', 'Filetype', 'Polarization',\n",
    "                'Coeffs', 'Modes', 'Models', 'Score', 'RMSE', 'Reof_forecast', 'Input_Images',\n",
    "                'Reof_hindcast', 'Hindcast', 'Forecast', 'Best_Index', 'Difference',\n",
    "                'Water_Hindcast', 'Water_Forecast'\n",
    "            ]\n",
    "        }\n",
    "        for key2 in ['Polynomial', 'Neural']\n",
    "    }\n",
    "    for key1 in ['Q', 'ERA']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Load the time template, every file combination has the same\n",
    "tiff_dir = Path(fc.selected)/'RTC_GAMMA'\n",
    "tiffs = list(tiff_dir.glob(f'*VV.tif*'))\n",
    "\n",
    "times = get_dates(tiff_dir, '*VV')\n",
    "times.sort()\n",
    "times = pd.DatetimeIndex(times)\n",
    "times.name = \"time\"\n",
    "\n",
    "# Create a figure showing the average water extent throughout the timeseries to identify when floods occured\n",
    "pathfig = Path(fc.selected)/'Figures'\n",
    "\n",
    "floodpercent = np.load(pathfig/\"flood_percentage.npy\")\n",
    "time_index = np.load(pathfig/\"time_index.npy\")\n",
    "time_index = pd.DatetimeIndex(time_index)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "import matplotlib.dates as mdates\n",
    "fig, ax = plt.subplots(figsize=(9,5))\n",
    "\n",
    "ax.plot(np.arange(len(np.unique(time_index))), floodpercent, color='b', marker='o', markersize=3, label='Area Covered in Water [%]')\n",
    "# Get the positions of the major tick locators on the x-axis\n",
    "ax.set_ylim([np.min(floodpercent)-np.min(floodpercent)*0.1, np.max(floodpercent)+np.min(floodpercent)*0.1])\n",
    "ax.set_xlabel('Date')\n",
    "\n",
    "\n",
    "ax.axhline(y=np.mean(floodpercent), color='k', linestyle='--', label='Average Water Coverage [%]')\n",
    "ax.set_ylabel('Image Area Covered in Water [%]')\n",
    "ax.plot([np.min(floodpercent)-np.min(floodpercent)*0.1, np.max(floodpercent)+np.min(floodpercent)*0.1])\n",
    "ax.grid()\n",
    "figname = ('ThresholdAndAreaTS.png')\n",
    "ax.legend(loc='lower right')\n",
    "\n",
    "# Set the x-axis tick positions\n",
    "x_positions = np.arange(len(np.unique(time_index)))\n",
    "major_tick_positions = ax.get_xticks()\n",
    "dates_string = [None]\n",
    "for i in range(1, len(major_tick_positions)-2):\n",
    "    dates_string.append(str(time_index[int(major_tick_positions[i])])[:7])\n",
    "dates_string.append(None)\n",
    "dates_string.append(None)\n",
    "'''\n",
    "# Annotate the plot with the text from dates_string\n",
    "for i, date_str in zip(x_positions, dates_string):\n",
    "    ax.annotate(date_str, xy=(i, 1.01), xycoords='data', ha='right', va='bottom')\n",
    "'''\n",
    "\n",
    "\n",
    "plt.title(f\"Maximum water coverage on {time_index[np.argmax(floodpercent)].strftime('%Y-%m-%d')}, index: {np.argmax(floodpercent)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4f77cb-19d8-4018-b80c-c33c3c45880d",
   "metadata": {},
   "source": [
    "**Choose the time window for the forecast**\n",
    "\n",
    "The index of the event you want to test.\n",
    "\n",
    "**IMPORTANT:** if using the indices, choose -1 before your index of interest.\n",
    "\n",
    "Example: the maximum water coverage occurs at index 50. Then I need to set sdaye = 49, edate = 50 so I can test the forecast for the date of the maximum water coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526d11e8-873b-42da-ad91-913828c7e772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "print(\"Choose a starting index and an end index (example: 10,11). Make sure you put a comma WITHOUT blank space.\")\n",
    "\n",
    "while True:\n",
    "    indices = input()\n",
    "    inds = indices.split(',')\n",
    "    start_ind = int(inds[0])\n",
    "    stop_ind = int(inds[1])\n",
    "    print(f\"the dates you selected are {str(time_index[start_ind])[:10]} to {str(time_index[stop_ind])[:10]}\")\n",
    "    \n",
    "    if (times[stop_ind]-times[start_ind]).days != 12:\n",
    "        print(f\"Your forecast is {(times[edate]-times[sdate]).days} days long, choose other indices. (most of the indices have 12 days intervals, try until it works)\")\n",
    "        continue\n",
    "\n",
    "    # Validate the input format using a regular expression\n",
    "    if not re.match(r'^\\d+,\\d+$', indices):\n",
    "        print(\"Please choose an appropriate input. It needs to be a number, a comma, and a number. No blank space.\")\n",
    "        continue\n",
    "    \n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f7af85-49bb-47a1-9fac-cca03e1c8196",
   "metadata": {},
   "source": [
    "**Choose the polynomial metric ranking**\n",
    "\n",
    "Choices:\n",
    "- ('rmse','min')\n",
    "- ('r2','max')\n",
    "- ('r','max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0669611c-8810-4523-a549-bf0f2fce5e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your metric ranking\n",
    "metricranking = ('r2','max')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f609411b-9f94-4056-b7c0-27ec29563b6e",
   "metadata": {},
   "source": [
    "### **Load the input variables**\n",
    "\n",
    "This part of the script is where we load the input variables. If you want to add variables, make sure they are in the dataarray format (usually from .nc files).\n",
    "\n",
    "They should have 1 value per day so they can be used by the functions we have.\n",
    "If you want to add a variable, use the \"mz\" variable list. \n",
    "\n",
    "For example: I want to add another precipitation model. I will modify the code as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2509471f-e168-4d05-9688-37cbfff6a081",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- Example \"mz\" modification:\n",
    "#mz = {\n",
    "#    'Q': {\n",
    "#        'Total': None,\n",
    "#        'Selected': None,\n",
    "#        'Forecast':None\n",
    "#    },\n",
    "#    'ERA': {\n",
    "#        'Total': None,\n",
    "#        'Selected': None,\n",
    "#        'Forecast':None\n",
    "#    },\n",
    "#    'New_Model': {\n",
    "#        'Total': None,\n",
    "#        'Selected': None,\n",
    "#        'Forecast':None\n",
    "#    },   \n",
    "#}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###### ------ LOAD THE INPUT VARIABLES ------ ######\n",
    "\n",
    "\n",
    "#   ---   Discharge from GEOGLOWS   ---   #\n",
    "\n",
    "\n",
    "# Convert 'times' to a dataarray to use with match_dates()\n",
    "time_dataarray = xr.DataArray(np.array(times), dims='time', coords={'time': np.array(times)})\n",
    "\n",
    "# Create MATCH, the list gathering the discharge and precipitations\n",
    "mz = {\n",
    "    'Q': {\n",
    "        'Total': None,\n",
    "        'Selected': None,\n",
    "        'Forecast':None\n",
    "    },\n",
    "    'ERA': {\n",
    "        'Total': None,\n",
    "        'Selected': None,\n",
    "        'Forecast':None\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "# Load in memory the discharge  dataset (will be the same everytime)\n",
    "lon,lat = get_centerpoint_coordinates(tiffs[0])\n",
    "mz['Q']['Total'] = fierpy.get_streamflow(lat,lon)\n",
    "mz['Q']['Selected'] = fierpy.match_dates(mz['Q']['Total'], time_dataarray[:start_ind])\n",
    "\n",
    "\n",
    "#   ---   Precipitation from ERA5   ---   #\n",
    "\n",
    "\n",
    "# First we need to get the coordinates of our AOI in order to crop the ERA5 input to our AOI\n",
    "\n",
    "# Get the projection of the AOI\n",
    "info = gdal.Info(str(tiffs[0]), format='json')\n",
    "info = info['coordinateSystem']['wkt']\n",
    "utm = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "\n",
    "\n",
    "# Get the coordinates of the AOI\n",
    "geotiff_path = str(tiffs[0])\n",
    "boundaries = get_coordinates_from_geotiff_bbox(geotiff_path)\n",
    "for idx, corner in enumerate(boundaries, 1):\n",
    "    lat, lon = corner\n",
    "    print(f\"Sanity Check: the coordinates of your AOI are: Corner {idx}: Latitude: {lat}, Longitude: {lon}\")\n",
    "    \n",
    "# Then we can open out netcdf file\n",
    "\n",
    "# Open Dataarray\n",
    "era_ds = xr.open_dataarray(f\"{Path(fc.selected).parent}/ERA5/era5_data.nc\")\n",
    "\n",
    "# Get the closest lats and lons from the AOI's boundaries, otherwise slicing the dataset can return empty slices\n",
    "latmin = era_ds['latitude'].values[np.argmin(np.abs(era_ds['latitude'].values - boundaries[0,0]))]\n",
    "latmax = era_ds['latitude'].values[np.argmin(np.abs(era_ds['latitude'].values - boundaries[1,0]))]\n",
    "lonmin = era_ds['longitude'].values[np.argmin(np.abs(era_ds['longitude'].values - boundaries[0,1]))]\n",
    "lonmax = era_ds['longitude'].values[np.argmin(np.abs(era_ds['longitude'].values - boundaries[1,1]))]\n",
    "\n",
    "# Loading the precipitation and just grabbing the value at the same day at we have a SAR image acquisition is wrong.\n",
    "# We try to alleviate that by summing the daily precipitation in between each SAR image, and adjusting that value based on the water extent trend.\n",
    "# If the water extent between two images is going down, the total precipitation between two images is its sum multiplied by the coefficient of the\n",
    "# slope between the two values of flooded area for the image at the time of the SAR acquisition.\n",
    "# For example: if the flooded area percentage is 5 on the 01/01, and 2.5 on the 01/05, then the coefficient will be 0.5.\n",
    "\n",
    "\n",
    "# Load in memory the precipitation dataset, fitted to the AOI, as a dataarray so it can use the match_dates function\n",
    "era_ds = era_ds.sel(longitude = slice(lonmin, lonmax), latitude = slice(latmax, latmin))\n",
    "\n",
    "# Convert to dataarray (sometimes ERA5 will give 2 levels of precipitation: ground and 5m altitude. We automatically take ground level.\n",
    "if len(era_ds.shape) == 3:\n",
    "    era_ds = np.sum(np.sum(era_ds, axis = 2), axis = 1)\n",
    "else:\n",
    "    era_ds = np.sum(np.sum(era_ds[:,0,:,:], axis = 2), axis = 1)\n",
    "\n",
    "# Select the indices of era that intersect the input images\n",
    "inds_match = era_ds.time.searchsorted(time_dataarray[:start_ind].time)\n",
    "\n",
    "# Calculate the interval of days between each SAR acquisition\n",
    "days_interval = inds_match[1:] - inds_match[:-1]\n",
    "\n",
    "# There sometimes are gaps in SAR acquisitions of multiple months. Because precipitations are sums, we need to correct that to avoid having one day with meters of cumulated rainfall.\n",
    "# In vast majority, repeat time of SAR acquisitions is 12 days. But we can directly calculate the median of our intervals and then divide our intervals by this median.\n",
    "# This will give us a correction factor for the cumulated precipitations.\n",
    "interval_correction = days_interval / 12\n",
    "\n",
    "# Prepare the cumulative sums array \n",
    "cumulative_sum = np.zeros(len(inds_match))\n",
    "\n",
    "# Calculate its first value by summing all the values before our first date, in the appropriate days interval\n",
    "cumulative_sum[0] = np.sum(era_ds.isel(time=slice(inds_match[0]-int(np.median(days_interval)), inds_match[0])).values)\n",
    "\n",
    "# Calculate the corrected precipitations by summing them between each acquisition, correct them with the sloop of flood maps and the amount of days in the interval\n",
    "for i in range(1, len(inds_match)):\n",
    "    # Apply a runoff based on the values of flood percentage of the images. If the trend is decreasing flood percentage, precipitation values decrease. If not, they stay the same\n",
    "    if (floodpercent[i] / floodpercent[i-1]) > 1:\n",
    "        runoff = 1\n",
    "    else:\n",
    "        runoff = floodpercent[i] / floodpercent[i-1]\n",
    "    cumulative_sum[i] = (np.sum(era_ds.isel(time=slice(inds_match[i]-days_interval[i-1], inds_match[i])).values)*runoff) / interval_correction[i-1]\n",
    "\n",
    "# Create a new DataArray with cumulative sum values\n",
    "era_sel = xr.DataArray(cumulative_sum, dims=('time'), coords={'time': time_dataarray[:start_ind].time})\n",
    "\n",
    "\n",
    "\n",
    "#   ---   Fill the Variables list   --- #\n",
    "mz['ERA']['Total'] = era_ds\n",
    "mz['ERA']['Selected'] = era_sel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a0cda6-d62e-479d-a57e-1f621317a874",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Looper**\n",
    "\n",
    "This part of the code loops through SAR VV and Water Masks VV + VH and calculates the REOF fits with the variables. It saves the scores, and will later determine which combination of input/variable/fit is the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db734be-e9ec-4d62-a124-391ca384cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store REOFs and Datasets\n",
    "reofs = []\n",
    "datasets = []\n",
    "\n",
    "\n",
    "# Iterate through every dataset type and polarization to compute the fits with the discharge\n",
    "c = 1\n",
    "for filetype, polarization in zip(list_types, list_polarization):\n",
    "        reof_ds, da, vv = looper(mz, filetype,\n",
    "                                            polarization,\n",
    "                                            start_ind,\n",
    "                                            metricranking,\n",
    "                                            vz,\n",
    "                                            fitting=True,\n",
    "                                            plotting = True\n",
    "                                            )\n",
    "        reofs.append(reof_ds)\n",
    "        datasets.append(da)\n",
    "        print(f\"{c}/2 | Finished computing {filetype} {polarization}\")\n",
    "        c += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1976b80-8bad-4b91-950d-c9cf02fba8a6",
   "metadata": {},
   "source": [
    "**Plot an empty figure to ensure the transition from interactive plots and inline plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1139e-c492-4c51-baa5-6f46fb6079cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(plt)\n",
    "%matplotlib inline\n",
    "reload(plt)\n",
    "%matplotlib inline\n",
    "plt.figure()\n",
    "plt.close()\n",
    "plt.figure()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce108d73-f1fd-413e-a34b-3f3d166a1559",
   "metadata": {
    "tags": []
   },
   "source": [
    "## **Visualize the fits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9e8b69-28c1-4670-8244-dae7c0a89ba6",
   "metadata": {},
   "source": [
    "**Select the best mode and associated coefficients based on the ranking score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72081115-f4e5-4b7c-8fee-953868789dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in ['Q','ERA']:\n",
    "    \n",
    "    # Depending on the metric, best score might be min or max value in the score list\n",
    "    if metricranking[1] == 'min':\n",
    "        vz[var]['Polynomial']['Best_Index'] = min(range(len(vz[var]['Polynomial']['Score'])), key=lambda i: abs(vz[var]['Polynomial']['Score'][i]))\n",
    "    else:\n",
    "        vz[var]['Polynomial']['Best_Index'] = max(range(len(vz[var]['Polynomial']['Score'])), key=lambda i: abs(vz[var]['Polynomial']['Score'][i]))\n",
    "    # Determine the filetype and polarization of best input\n",
    "    vz[var]['Polynomial']['Filetype'] = list_types[vz[var]['Polynomial']['Best_Index']]\n",
    "    vz[var]['Polynomial']['Polarization'] = list_polarization[vz[var]['Polynomial']['Best_Index']]\n",
    "        \n",
    "    # Do the same for the neural network\n",
    "    vz[var]['Neural']['Best_Index']  = vz[var]['Neural']['RMSE'].index(min(vz[var]['Neural']['RMSE']))\n",
    "    vz[var]['Neural']['Filetype'] = list_types[vz[var]['Neural']['Best_Index']]\n",
    "    vz[var]['Neural']['Polarization'] = list_polarization[vz[var]['Neural']['Best_Index']]\n",
    "    \n",
    "    # Replace the coeffs, modes and models by their best iteration\n",
    "    vz[var]['Polynomial']['Coeffs'] = np.array(vz[var]['Polynomial']['Coeffs'][vz[var]['Polynomial']['Best_Index']])\n",
    "    vz[var]['Polynomial']['Modes'] = vz[var]['Polynomial']['Modes'][vz[var]['Polynomial']['Best_Index']]\n",
    "    vz[var]['Neural']['Models'] = vz[var]['Neural']['Models'][vz[var]['Neural']['Best_Index']]\n",
    "    \n",
    "    # Get the filetype and polarization of best score of polynomial\n",
    "    newpola = [f\"{vz[var]['Polynomial']['Filetype']} {vz[var]['Polynomial']['Polarization']}\"]\n",
    "    newpola.append(f\"{vz[var]['Neural']['Filetype']} {vz[var]['Neural']['Polarization']}\")\n",
    "    \n",
    "    # Attribute to each variable and method its optimal reof and dataset\n",
    "    if vz[var]['Polynomial']['Filetype'] == \"SAR\":\n",
    "        vz[var]['Polynomial']['Reof'], vz[var]['Polynomial']['Training_Data'] = reofs[0], datasets[0]\n",
    "    else: # If not SAR, Water_Masks is the best dataset along with its REOF\n",
    "        vz[var]['Polynomial']['Reof'], vz[var]['Polynomial']['Training_Data'] = reofs[1], datasets[1]\n",
    "                   \n",
    "    # Do the same for Neural method\n",
    "    if vz[var]['Neural']['Filetype'] == \"SAR\":\n",
    "        vz[var]['Neural']['Reof'], vz[var]['Neural']['Training_Data'] = reofs[0], datasets[0]\n",
    "    else:\n",
    "        vz[var]['Neural']['Reof'], vz[var]['Neural']['Training_Data'] = reofs[1], datasets[1]\n",
    "    \n",
    "    # Fill the hindcasts               \n",
    "    vz[var]['Polynomial']['Hindcast'] = vz[var]['Polynomial']['Training_Data'][start_ind:stop_ind+1]\n",
    "    vz[var]['Neural']['Hindcast'] = vz[var]['Neural']['Training_Data'][start_ind:stop_ind+1]\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedf1de5-cc44-49da-853a-df76e85b9e70",
   "metadata": {},
   "source": [
    "**Prepare the datasets for the fit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902fd569-cdf2-4f58-8953-d129e0cda473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fill in the NaNs to avoid breaking the fit\n",
    "for var, reg in zip(['Q','ERA'],['Polynomial','Neural']):\n",
    "    vz[var][reg]['Reof'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291113e3-b8e3-4847-97b2-fc367709affa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(plt)\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(15,5), ncols=2)\n",
    "\n",
    "xlabels = ['Discharge [$m^{3}.s^{-1}$]', 'Precipitations [$m.d^{-1}$]']\n",
    "for var, i in zip(['Q','ERA'],range(2)):\n",
    "    vals = vz[var]['Polynomial']\n",
    "    x = np.linspace(0, mz[var]['Selected'].max(),100)\n",
    "    f = np.poly1d(np.squeeze(vals['Coeffs']))\n",
    "    ax[i].plot(mz[var]['Selected'],vals['Reof'].temporal_modes[:,vals['Modes']-1], 'o', label=f\"{metricranking[0]} = {np.round(vals['Score'][vals['Best_Index']],5)}\")\n",
    "    ax[i].legend()\n",
    "    ax[i].set_xlabel(xlabels[i])\n",
    "    ax[i].set_ylabel('Time series amplitude')\n",
    "    ax[i].plot(x,f(x))\n",
    "    ax[i].set_title(f\"Fit for mode {vals['Modes']}, poly degree {len(vals['Coeffs'])-1}, {vals['Filetype']}_{vals['Polarization']}\")            \n",
    "plt.savefig(pathfig/f\"Polynomial_Fit_{times[0].strftime('%Y-%m-%d')}_{times[start_ind].strftime('%Y-%m-%d')}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a005bf50-b7f0-4658-87eb-761c4cb44129",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(constrained_layout=True, figsize=(15, 5))\n",
    "\n",
    "row_titles = [f\"Discharge | {vz['Q']['Neural']['Filetype']}_{vz['Q']['Neural']['Polarization']}, RMSE: {min(vz['Q']['Neural']['RMSE']):.3f}\",\n",
    "              f\"Precipitation | {vz['ERA']['Neural']['Filetype']}_{vz['ERA']['Neural']['Polarization']}, RMSE: {min(vz['ERA']['Neural']['RMSE']):.3f}\"]\n",
    "\n",
    "# Create 3x1 subfigs\n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(15,15))\n",
    "\n",
    "for var, i in zip(['Q','ERA'],range(2)):\n",
    "    ax[i].set_ylabel('Temporal Mode Amplitude', color='black')\n",
    "    ax[i].yaxis.set_label_coords(-0.2, 0.5)\n",
    "    \n",
    "    for m in range(vz[var]['Neural']['Reof'].temporal_modes.shape[1]):\n",
    "        ax[i].scatter(mz[var]['Selected'], vz[var]['Neural']['Reof'].temporal_modes[:, m])\n",
    "        ax[i].plot(mz[var]['Selected'], vz[var]['Neural']['Models'][m].predict(mz[var]['Selected'].values), label=f\"{m}\")\n",
    "        ax[i].set_xlabel(xlabels[i])\n",
    "    \n",
    "    ax[i].set_title(f\"Neural Network | {vz[var]['Neural']['Reof'].temporal_modes.shape[1]} modes | {vz[var]['Neural']['Filetype']} {vz[var]['Neural']['Polarization']}\")         \n",
    "fig.savefig(pathfig/f\"Neural_fit_{times[0].strftime('%Y-%m-%d')}_{times[start_ind].strftime('%Y-%m-%d')}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854be79-5570-4bae-8a61-5d987b19271b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "row_titles = [f\"Discharge | {vz['Q']['Neural']['Filetype']}_{vz['Q']['Neural']['Polarization']}, RMSE: {min(vz['Q']['Neural']['RMSE']):.3f}\",\n",
    "              f\"Precipitation | {vz['ERA']['Neural']['Filetype']}_{vz['ERA']['Neural']['Polarization']}, RMSE: {min(vz['ERA']['Neural']['RMSE']):.3f}\"]\n",
    "\n",
    "\n",
    "width_per_subplot = 5  # Desired width (in inches) for each subplot\n",
    "num_columns = len(vz[var]['Polynomial']['Reof'].mode.values)\n",
    "fig_width = num_columns * width_per_subplot\n",
    "\n",
    "fig = plt.figure(constrained_layout=True, figsize=(15, 10))\n",
    "\n",
    "labels = ['Discharge','Precipitation']\n",
    "# Create 3x1 subfigs\n",
    "subfigs = fig.subfigures(nrows=2, ncols=1)\n",
    "\n",
    "for var, i in zip(['Q','ERA'],range(2)):\n",
    "    subfigs[i].suptitle(f'{row_titles[i]}')\n",
    "\n",
    "    # Create 1x3 subplots per subfig\n",
    "    axs = subfigs[i].subplots(nrows=1, ncols=4)\n",
    "    axs[i].set_ylabel('Normalized Values', color='black')\n",
    "    axs[i].yaxis.set_label_coords(-0.2, 0.5)\n",
    "    \n",
    "    for col, ax in enumerate(axs):\n",
    "        reof_norm = normalize(vz[var]['Polynomial']['Reof'].temporal_modes[:, col])\n",
    "        vari = normalize(mz[var]['Selected'])\n",
    "        # Plotting the data with modified labels and line styles\n",
    "        ax.plot(times[:start_ind], (reof_norm - np.min(reof_norm)) / (np.max(reof_norm) - np.min(reof_norm)),\n",
    "                        label='RTPC', linestyle='-', linewidth=2, color='black')  # RTPC - Red solid line\n",
    "        ax.plot(times[:start_ind], (vari - np.min(vari)) / (np.max(vari) - np.min(vari)),\n",
    "                        label=labels[i], linestyle='--', linewidth=2, dashes=(5, 2), color='blue')  # Discharge - Dashed line\n",
    "        ax.set_xlabel('Times')  # X-axis label\n",
    "        ax.set_ylabel('Normalized Values')  # Y-axis label\n",
    "        corr, _ = pearsonr(reof_norm, vari)\n",
    "        ax.set_title(f'Mode {col+1} (Corr: {corr:.3f})', fontsize=20)  # Subplot title with correlation\n",
    "        ax.legend(fontsize=17)  # Show legend\n",
    "        ax.tick_params(axis='x', rotation=45)  # Rotate x-axis tick labels by 45 degrees\n",
    "\n",
    "\n",
    "#plt.tight_layout()  # Adjust the layout spacing\n",
    "plt.show()  # Display the plot\n",
    "plt.savefig(pathfig/f\"Variables_Modes_Correlations_{times[0].strftime('%Y-%m-%d')}_{times[start_ind].strftime('%Y-%m-%d')}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c794508b-f169-4ddc-946b-df31762489e3",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738fd17-6bb8-49a3-80db-4b041093c459",
   "metadata": {},
   "source": [
    "## **Forecast**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0a7cc-133e-421e-946a-4001c70a2604",
   "metadata": {},
   "source": [
    "#### **Calculate the forecast of flooding based on the relationship between discharge and the main modes of our dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bce77b-cd80-41f6-bbb9-f4b33a236a7a",
   "metadata": {},
   "source": [
    "**Calculate the amount of days between sdate and edate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73581bf-0e9a-4b88-9d0a-725b9aeb7b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of days for the forecast\n",
    "nb_days_forecast = (times[stop_ind] - times[start_ind]).days"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd0d1b2-cdb4-4f13-b20d-6a53c4d1b255",
   "metadata": {},
   "source": [
    "**Generate the forecast based on the best fit coefficients**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5320793-02e3-45c1-afe6-0927905eb8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new dates array nb_days_forecast days into the future and save them as a dataset\n",
    "forecast_dates = xr.Dataset.from_dataframe(\n",
    "                    pd.DataFrame(\n",
    "                        {'time': da[start_ind].time.values +\n",
    "                         np.arange(1, nb_days_forecast+1, dtype='timedelta64[D]')}\n",
    "                    )\n",
    "                    )\n",
    "\n",
    "for var in ['Q','ERA']:\n",
    "    mz[var]['Forecast'] = fierpy.match_dates(mz[var]['Total'],forecast_dates)\n",
    "    \n",
    "    for reg in ['Polynomial','Neural']:\n",
    "        # Use the previously found relationships and functions to generate RTC/mask forecast\n",
    "        vz[var]['Polynomial']['Forecast'] = fierpy.synthesize(vz[var]['Polynomial']['Reof'],\n",
    "                                                              mz[var]['Forecast'],\n",
    "                                                              np.poly1d(vz[var]['Polynomial']['Coeffs']),\n",
    "                                                              vz[var]['Polynomial']['Modes'])\n",
    "        \n",
    "        vz[var]['Neural']['Forecast'] = synthesize_neural(mz[var]['Forecast'],\n",
    "                                                          vz[var]['Neural']['Models'],\n",
    "                                                          vz[var]['Neural']['Reof'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f306a89-26fd-48c2-bf40-d6ce7fb0c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoHANDLayerException(Exception):\n",
    "    \"\"\"\n",
    "    Raised when expecting path to HAND layer but none found\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "#load HAND and derive HAND-EM\n",
    "Hthresh = 15\n",
    "\n",
    "path_HAND = Path(fc.selected)/'DEM/'\n",
    "HAND_file = list(path_HAND.glob(f'*hand*'))[0]\n",
    "\n",
    "print(f\"Selected HAND: {HAND_file}\")\n",
    "try:\n",
    "    HAND_gT = gdal_get_geotransform(HAND_file)\n",
    "except AttributeError:\n",
    "    raise NoHANDLayerException(\"You must select a HAND layer in the previous code cell\")\n",
    "info = (gdal.Info(str(HAND_file), format='json'))\n",
    "info = info['coordinateSystem']['wkt']\n",
    "Hzone = info.split('ID')[-1].split(',')[1][0:-2]\n",
    "Hproj_type = info.split('ID')[-1].split('\"')[1]\n",
    "HAND=gdal_read(HAND_file)\n",
    "hand = np.nan_to_num(HAND)\n",
    "\n",
    "# Create Binary HAND-EM\n",
    "Hmask = hand < Hthresh\n",
    "handem = np.zeros_like(hand)\n",
    "sel = np.ones_like(hand)\n",
    "handem[Hmask] = sel[Hmask]\n",
    "\n",
    "\n",
    "# Prepare the arrays to host the scores results\n",
    "CSI=np.zeros((4))\n",
    "Overall = np.zeros((4))\n",
    "Precision = np.zeros((4))\n",
    "Recall = np.zeros((4))\n",
    "\n",
    "# Calculate the scores\n",
    "i = 0\n",
    "for var in ['Q','ERA']:\n",
    "    for reg in ['Polynomial','Neural']:\n",
    "        Overall[i], CSI[i], Precision[i], Recall[i], vz[var][reg]['Water_Forecast'], vz[var][reg]['Water_Hindcast'] = Z_Score_Iterative(vz, var, reg, handem)\n",
    "        print(f\"#### ---- Finished computing {var} {reg} ---- ####\")\n",
    "        i += 1\n",
    "\n",
    "# Define the row and column labels\n",
    "row_labels = ['Discharge', 'Precipitation']\n",
    "column_labels = ['Polynomial', 'Neural']\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Plot the matrix using imshow\n",
    "im = ax.imshow(np.array([precision-recall for precision, recall in zip(Precision, Recall)]).reshape((2,2)), cmap='RdYlGn')\n",
    "\n",
    "# Add ticks and labels to the x-axis and y-axis\n",
    "ax.set_xticks(np.arange(len(column_labels)))\n",
    "ax.set_yticks(np.arange(len(row_labels)))\n",
    "ax.set_xticklabels(column_labels)\n",
    "ax.set_yticklabels(row_labels)\n",
    "\n",
    "p = 0\n",
    "# Loop over the data to add text annotations in each cell\n",
    "for i in range(len(row_labels)):\n",
    "    for j in range(len(column_labels)):\n",
    "        text = ax.text(j, i, f\"CSI: {CSI[p]:.2f}\\nTot: {Overall[p]:.2f}\\nPrecision: {Precision[p]:.2f}\\nRecall: {Recall[p]:.2f}\", ha='center', va='center', color='white')\n",
    "        p += 1\n",
    "\n",
    "# Set the title\n",
    "ax.set_title(\"Forecast Scores\")\n",
    "\n",
    "# Add a colorbar\n",
    "cbar = ax.figure.colorbar(im, label='Precision-Recall Score [%]')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "fig.savefig(pathfig/f\"Scores_{times[0].strftime('%Y-%m-%d')}_{times[start_ind].strftime('%Y-%m-%d')}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df73188-87c2-4ca9-a785-2a1f8d714faa",
   "metadata": {},
   "source": [
    "### **Plot the forecasts and their associated real data to compare spatial correspondance**\n",
    "*The colorbar is adjusted to the values of each normalized dataset. We use as colorbar boundaries:\n",
    "$\\mu(\\text{dataset})\\pm\\sigma(\\text{dataset})$*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a98f42-2198-45fb-9cd8-f99485f4b707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically set the size of the plot\n",
    "\n",
    "names = ['Discharge','','Precipitation']\n",
    "\n",
    "for reg, r in zip(['Polynomial', 'Neural'], range(2)):\n",
    "    \n",
    "    ncols = 1  # Number of columns in your subplots\n",
    "    nrows = 4\n",
    "    width_ratios = [1] * ncols  # Equal width for subplots, and additional space for colorbar\n",
    "    width_ratios[-1] += 0.075\n",
    "    height_ratios = [1] * nrows\n",
    "\n",
    "    width_per_subplot = 5  # Desired width (in inches) for each subplot\n",
    "    fig_width = num_columns * width_per_subplot\n",
    "\n",
    "    # Start Figure\n",
    "    fig, ax = plt.subplots(figsize=(20, 20), nrows = 4, ncols = 1, gridspec_kw={'width_ratios': width_ratios, 'height_ratios': height_ratios})\n",
    "    \n",
    "    # Create title\n",
    "    #fig.suptitle(f\"Q: {vz['Q'][reg]['Filetype']} {vz['Q'][reg]['Polarization']}, mode {vz['Q'][reg]['Modes']}, poly deg {len(vz['Q'][reg]['Coeffs'])-1} | Prec: {vz['ERA'][reg]['Filetype']} {vz['ERA'][reg]['Polarization']}, mode {vz['ERA'][reg]['Modes']}, poly deg{len(vz['ERA'][reg]['Coeffs'])-1}\", fontsize = 18)\n",
    "\n",
    "\n",
    "    for var, j in zip(['Q', 'ERA'], [0,2]):\n",
    "        forecast = vz[var][reg]['Forecast'][-1]\n",
    "        hindcast = vz[var][reg]['Hindcast'][0]\n",
    "\n",
    "\n",
    "        # Loop to plot the data. We use the forecast and their associated real data, normalized\n",
    "        # The colorbar is varying from -std+mean to std+mean\n",
    "           \n",
    "\n",
    "        plt.figure()\n",
    "        forecast = np.ma.masked_where(forecast == 0, forecast)\n",
    "        hindcast = np.ma.masked_where(hindcast == 0, hindcast)\n",
    "\n",
    "        im0 = ax[j].imshow(hindcast, cmap='magma', interpolation=\"none\", vmin=0, vmax=1.2)\n",
    "        ax[0].set_title(f\"{reg} {str(vz[var][reg]['Forecast'][i].time.values)[0:10]}, d{nb_days_forecast}\", fontsize = 17)\n",
    "        im1 = ax[j+1].imshow(forecast, cmap='magma', interpolation=\"none\", vmin=0, vmax=1.2)\n",
    "\n",
    "        ax[j].set_xticks([])\n",
    "        ax[j].set_yticks([])\n",
    "        ax[j+1].set_xticks([])\n",
    "        ax[j+1].set_yticks([])\n",
    "\n",
    "        ax[j].set_ylabel(f\"Hindcast {vz[var][reg]['Filetype']} {vz[var][reg]['Polarization']}\", fontsize = 15)\n",
    "        ax[j+1].set_ylabel(f\"Forecast {vz[var][reg]['Filetype']} {vz[var][reg]['Polarization']}\", fontsize = 15)\n",
    "                \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "    # rearange the axes for no overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Plot line in the middle\n",
    "    line = plt.Line2D([0,1],[0.495,0.495], transform=fig.transFigure, color=\"red\")\n",
    "    # Create the first title for the first two rows\n",
    "    title1 = fig.text(0.1, 0.75, 'Discharge', va='center', ha='center', rotation='vertical', color='red', fontsize = 20)\n",
    "\n",
    "    # Create the second title for the last two rows\n",
    "    title2 = fig.text(0.1, 0.25, 'Precipitations', va='center', ha='center', rotation='vertical', color='red', fontsize = 20)\n",
    "\n",
    "    fig.add_artist(line)\n",
    "\n",
    "\n",
    "    fig.savefig(pathfig/f\"Forecast_Hindcast_{reg}_{times[0].strftime('%Y-%m-%d')}_{times[start_ind].strftime('%Y-%m-%d')}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0f8db4-7893-47ae-b12e-2934f1570047",
   "metadata": {},
   "source": [
    "**Compare the water masks from the Z-score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec31ec2-b3d4-4e70-bf48-fa1e97d00b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically set the size of the plot\n",
    "for reg, r in zip(['Polynomial', 'Neural'], range(2)):\n",
    "    \n",
    "    ncols = 1  # Number of columns in your subplots\n",
    "    nrows = 4\n",
    "    width_ratios = [1] * ncols  # Equal width for subplots, and additional space for colorbar\n",
    "    width_ratios[-1] += 0.075\n",
    "    height_ratios = [1] * nrows\n",
    "\n",
    "    width_per_subplot = 5  # Desired width (in inches) for each subplot\n",
    "    fig_width = num_columns * width_per_subplot\n",
    "\n",
    "    # Start Figure\n",
    "    fig, ax = plt.subplots(figsize=(20, 20), nrows = 4, ncols = 1, gridspec_kw={'width_ratios': width_ratios, 'height_ratios': height_ratios})\n",
    "    \n",
    "    # Create title\n",
    "    #fig.suptitle(f\"Q: {vz['Q'][reg]['Filetype']} {vz['Q'][reg]['Polarization']}, mode {vz['Q'][reg]['Modes']}, poly deg {len(vz['Q'][reg]['Coeffs'])-1} | Prec: {vz['ERA'][reg]['Filetype']} {vz['ERA'][reg]['Polarization']}, mode {vz['ERA'][reg]['Modes']}, poly deg{len(vz['ERA'][reg]['Coeffs'])-1}\", fontsize = 18)\n",
    "\n",
    "\n",
    "    for var, j in zip(['Q', 'ERA'], [0,2]):\n",
    "        forecast = vz[var][reg]['Water_Forecast'][-1]\n",
    "        hindcast = vz[var][reg]['Water_Hindcast']\n",
    "\n",
    "\n",
    "        # Loop to plot the data. We use the forecast and their associated real data, normalized\n",
    "        # The colorbar is varying from -std+mean to std+mean\n",
    "           \n",
    "\n",
    "        plt.figure()\n",
    "        forecast = np.ma.masked_where(forecast == 0, forecast)\n",
    "        hindcast = np.ma.masked_where(hindcast == 0, hindcast)\n",
    "\n",
    "        im0 = ax[j].imshow(hindcast, cmap='Blues', interpolation=\"none\", vmin=0, vmax=1.2)\n",
    "        ax[0].set_title(f\"{reg} {str(vz[var][reg]['Forecast'][i].time.values)[0:10]}, d{nb_days_forecast}\", fontsize = 17)\n",
    "        im1 = ax[j+1].imshow(forecast, cmap='Blues', interpolation=\"none\", vmin=0, vmax=1.2)\n",
    "\n",
    "        ax[j].set_xticks([])\n",
    "        ax[j].set_yticks([])\n",
    "        ax[j+1].set_xticks([])\n",
    "        ax[j+1].set_yticks([])\n",
    "\n",
    "        ax[j].set_ylabel(f\"Water Hindcast {vz[var][reg]['Filetype']} {vz[var][reg]['Polarization']}\", fontsize = 15)\n",
    "        ax[j+1].set_ylabel(f\"Water Forecast {vz[var][reg]['Filetype']} {vz[var][reg]['Polarization']}\", fontsize = 15)\n",
    "\n",
    "\n",
    "    # rearange the axes for no overlap\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Plot line in the middle\n",
    "    line = plt.Line2D([0,1],[0.495,0.495], transform=fig.transFigure, color=\"red\")\n",
    "    # Create the first title for the first two rows\n",
    "    title1 = fig.text(0.1, 0.75, 'Discharge', va='center', ha='center', rotation='vertical', color='red', fontsize = 20)\n",
    "\n",
    "    # Create the second title for the last two rows\n",
    "    title2 = fig.text(0.1, 0.25, 'Precipitations', va='center', ha='center', rotation='vertical', color='red', fontsize = 20)\n",
    "\n",
    "    fig.add_artist(line)\n",
    "\n",
    "\n",
    "    fig.savefig(pathfig/f\"Water_Extent_{reg}_{times[0].strftime('%Y-%m-%d')}_{times[start_ind].strftime('%Y-%m-%d')}.png\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f2ebac-173a-46e7-a4c0-b93950ec355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add figure with runoff on top and water for each forecasted day\n",
    "# Animation to show SLC input\n",
    "\n",
    "for reg in ['Polynomial','Neural']:\n",
    "    for var in ['Q','ERA']:\n",
    "        fig = plt.figure(figsize=(15,15))\n",
    "        gs = fig.add_gridspec(4, 4)\n",
    "        ax1 = fig.add_subplot(gs[0, :])\n",
    "        ax11 = ax1.twinx()\n",
    "\n",
    "        q_min, q_max = np.where(mz['Q']['Total'].time == time_dataarray[start_ind].time)[0][0], np.where(mz['Q']['Total'].time == time_dataarray[stop_ind].time)[0][0]\n",
    "        era_min, era_max = np.where(mz['ERA']['Total'].time == time_dataarray[start_ind].time)[0][0], np.where(mz['ERA']['Total'].time == time_dataarray[stop_ind].time)[0][0]\n",
    "\n",
    "        days = [f\"d{i}\" for i in range(1,nb_days_forecast+1)]\n",
    "\n",
    "        ax1.plot(days, mz['Q']['Total'][q_min:q_max])\n",
    "        ax11.plot(days, mz['ERA']['Total'][era_min:era_max], color='red')\n",
    "        ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "        ax1.set_xlabel('Days forecast')\n",
    "        ax1.set_ylabel('Discharge [$m^{3}.s^{-1}$]')\n",
    "        ax11.set_ylabel('Precipitation [$m.d^{-1}$]')\n",
    "        plt.title(f\"Forecast from {vz[var][reg]['Filetype']} {vz[var][reg]['Polarization']}\")\n",
    "\n",
    "        # Plot n images from vz['Q']['Polynomimal']['Water_Forecast']\n",
    "        p = 0\n",
    "        for i in range(1,4):\n",
    "            for j in range(4):\n",
    "                forecast = vz[var][reg]['Water_Forecast'][p]\n",
    "                ax = fig.add_subplot(gs[i,j])  # Create subplots for each image\n",
    "                ax.imshow(forecast, cmap='Blues', interpolation=\"none\", vmin=0, vmax=1.2)\n",
    "                ax.set_title(f'd{p+1}')\n",
    "                ax.set_xticks([])  # Turn off xticks\n",
    "                ax.set_yticks([])  # Turn off yticks\n",
    "                ax.set_xlabel('')  # Turn off x-axis label\n",
    "                ax.set_ylabel('')  # Turn off y-axis label\n",
    "                p += 1\n",
    "\n",
    "        fig.savefig(pathfig/f\"Daily_WM_{reg}_{var}_{times[0].strftime('%Y-%m-%d')}_{times[start_ind].strftime('%Y-%m-%d')}.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hydrosar",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
