{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<div><img src=\"https://radar.community.uaf.edu/wp-content/uploads/sites/667/2021/03/HydroSARbanner.jpg\" width=\"100%\" /></div>\n",
    "\n",
    "**NASA A.37 Project:** Integrating SAR Data for Improved Resilience and Response to Weather-Related Disasters\n",
    "**PI:** Franz J. Meyer\n",
    "\n",
    "# HydroSAR Transition workshop\n",
    "\n",
    "## Archiving successful HyP3 jobs in an AWS S3 bucket\n",
    "\n",
    "In order to automatically archive newly created HydroSAR products we:\n",
    "1. Query HyP3 for all the successful HydroSAR jobs we've submitted\n",
    "2. Query our archive for all the products we've archived\n",
    "3. Deduplicate the products lists to determine the *new* products to archive\n",
    "4. Transfer the new products to our archive\n",
    "\n",
    "This notebook walks through the archiving process and would be run on a regular schedule (cron) in application.\n",
    "\n",
    "Note: Here we're taking the strategy to *always* search for **all** possible products, *always* look up **all** previously created products, and then *deduplicate* the two lists to determine the new products. You could instead keep track of the last time the script was *successfully* run and only search for new products since then. While this would be more performant, our strategy is independent of previous runs so is generally more fault-tolerant and can be started and stopped at will."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query HyP3 for all the HydroSAR products\n",
    "\n",
    "We use HyP3 as our workflow engine and can query for all the scenes we've already processed. Importantly, when we submit jobs, we will assign all of them a project name which is used to group jobs together and later search for them.\n",
    "\n",
    "First we need to specify our project name so that we can find all the jobs associated with the Area of Interest (AOI) we're monitoring"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "project_name = 'HKHwatermaps'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this notebook, we'll prompt for an Earthdata Login username and password, but they can be provided via the `username` and `password` keyword arguments, or automatically pulled from the users `~/.netrc` file.\n",
    "\n",
    "Note: Typically you'll want to use a shared \"operational\" Earthdata Login user as you can only search for jobs associated with your username."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hyp3_sdk as sdk\n",
    "hyp3 = sdk.HyP3('https://hyp3-watermap.asf.alaska.edu', prompt=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we'll search for all the jobs with our project name and filter to the succeeded jobs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "processed_jobs = hyp3.find_jobs(name=project_name)\n",
    "succeeded_jobs = processed_jobs.filter_jobs(running=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Query our archive for all the products we've archived\n",
    "\n",
    "For our archive, we are storing all the products under a project prefix in an S3 Bucket."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "archive_bucket = 'hyp3-nasa-disasters'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll build our set of archived products by querying the bucket."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "S3 = boto3.resource('s3')\n",
    "\n",
    "project_archive = set()\n",
    "for object in S3.Bucket(archive_bucket).objects.filter(Prefix=f'{project_name}/'):\n",
    "        project_archive.add(object.key)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Deduplicate the products lists to determine the *new* products to archive\n",
    "\n",
    "Every successful HyP3 `WATER_MAP` job will have created a set of HydroSAR products, each of which can be identified by its file suffix. First, we define the list products we'd like to archive:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "product_suffixes = ['_VV.tif', '_VH.tif', '_dem.tif', '_rgb.tif', '_HAND.tif', '_WM.tif', '_WaterDepth.tif', '_FloodDepth.tif']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Then we'll loop through all the succeeded jobs, and see if all the products we want to archive are in the project archive, to build the new set of products to archive."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "products_to_archive = []\n",
    "for job in tqdm(succeeded_jobs):\n",
    "    hyp3_product_bucket = job.files[0]['s3']['bucket']\n",
    "    hyp3_product_key = job.files[0]['s3']['key']\n",
    "    for sfx in product_suffixes:\n",
    "        source_key = hyp3_product_key.replace('.zip', sfx)\n",
    "        target_key = source_key.replace(job.job_id, project_name)\n",
    "        if target_key not in project_archive:\n",
    "            products_to_archive.append(\n",
    "                {\n",
    "                    'source_bucket': hyp3_product_bucket,\n",
    "                    'source_key': source_key,\n",
    "                    'target_bucket': archive_bucket,\n",
    "                    'target_key': target_key,\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 4. Transfer the new products to our archive\n",
    "\n",
    "Before we start transferring our new products, we can set up a more performant AWS S3 transfer configuration:g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from boto3.s3.transfer import TransferConfig\n",
    "\n",
    "chunk_size = 104857600\n",
    "transfer_config = TransferConfig(multipart_threshold=chunk_size, multipart_chunksize=chunk_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "And then begin the transfer of products from HyP3 to our archive."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for product in tqdm(products_to_archive):\n",
    "        bucket = S3.Bucket(product['target_bucket'])\n",
    "        copy_source = {'Bucket': product['source_bucket'], 'Key': product['source_key']}\n",
    "        bucket.copy(CopySource=copy_source, Key=product['target_key'], Config=transfer_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Operations\n",
    "\n",
    "To do all this operationally, you'll typically want to extract all the above configuration into a separate configuration file, and the python code into an easily executable script, and then execute the script regularly via a cron job (or similar) *somewhere*.\n",
    "\n",
    "In practice, ASF:\n",
    "* Has defined all the configuration in this file: https://github.com/ASFHyP3/hyp3-nasa-disasters/blob/main/data_management/hkh_watermaps.json\n",
    "* Scripted this notebook: https://github.com/ASFHyP3/hyp3-nasa-disasters/blob/main/data_management/hyp3_transfer_script.py\n",
    "* Executes the script every 6 hours using GitHub Actions \"scheduled\" (cron) workflows: https://github.com/ASFHyP3/hyp3-nasa-disasters/blob/main/.github/workflows/process-and-transfer.yml\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}